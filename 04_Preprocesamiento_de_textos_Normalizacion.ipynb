{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/NLP/blob/main/04_Preprocesamiento_de_textos_Normalizacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH5v3C5Gl8CC"
      },
      "source": [
        "# 04 - Preprocesamiento de textos (Normalización)\n",
        "\n",
        "* Antes de procesar los texto con cualquier algoritmo de aprendizaje automático (supervisado o no supervisado) es necesario realizar un preporcesamiento con el objetivo de limpiar, normalizar y estructurar el texto.\n",
        "\n",
        "\n",
        "* Para ello se propone el siguiente framework:\n",
        "\n",
        "\n",
        "<img src=\"./imgs/007_framework_preprocesamiento_texto.png\" style=\"width: 500px;\"/>\n",
        "\n",
        "\n",
        "* Los pasos propuestos en este framework pueden abordarse en el orden que se quiera e incluso alguno de estas etapas no sería necesario realizarse en función de como tengamos los textos.\n",
        "\n",
        "\n",
        "* Definamos a continuación lo que hay que realizar en cada uno de estos pasos:\n",
        "\n",
        "\n",
        "1.- ***Eliminación de ruido***:\n",
        "\n",
        "   * Este paso tiene como objetivo eliminar todos aquellos símbolos o caracteres que no aportan nada en el significado de las frases (ojo no confundir con las stop-words), como por ejemplo etiquetas HTML (para el caso del scraping), parseos de XML, JSON, etc.\n",
        "    \n",
        "2.- ***Tokenización***:\n",
        "   * Este paso tiene como objetivo dividir las cadenas de texto del documento en piezas más pequeñas o tokens.\n",
        "   * Aunque la tokenización es el proceso de dividir grandes cadenas de texto en cadenas más pequeñas, se suele diferenciar la:\n",
        "       * ***Segmentation***: Tarea de dividir grandes cadenas de texto en piezas más pequeñas como oraciones o párrafos.\n",
        "       * ***Tokenization***: Tarea de dividir grandes cadenas de texto solo y exclusivamente en palabras.\n",
        "    \n",
        "3.- ***Normalización***:\n",
        "\n",
        "   * La normalización es una tarea que tiene como objetivo poner todo el texto en igualdad de condiciones:\n",
        "        * Convertir todo el texto en mayúscula o minúsculas\n",
        "        * Eliminar, puntos, comas, comillas, etc.\n",
        "        * Convertir los números a su equivalente a palabras\n",
        "        * Quitar las Stop-words\n",
        "        * etc.\n",
        "        \n",
        "<hr>\n",
        "\n",
        "## Ejemplo de Preprocesamiento de Texto.\n",
        "\n",
        "\n",
        "* Aunque no hay una norma o guía de como realizar una normalización de texto ya que esta depende del problema a resolver y de la naturaleza del texto, vamos a mostrar a continuación algunas operaciones más o menos comúnes para la tokenización y normalización de los textos.\n",
        "\n",
        "\n",
        "* Si bien este ejemplo esta hecho utilizando la librería de ***spaCy*** (ya que lo vamos a aplicar sobre un texto en Español) puede realizarse tambien con la librería de ***NLTK*** e incluso determinadas funcionalidades de tratamiento de strings lo podemos hacer con otras librerías.\n",
        "\n",
        "\n",
        "* En el siguiente ejemplo vamos a tokenizar y normalizar un texto:\n",
        "    1. Transformar un texto en tokens\n",
        "    2. Eliminar los tokens que son signos (puntuación, exclamación, etc.)\n",
        "    3. Eliminar las palabras que tienen menos de 'N' caracteres\n",
        "    4. Eliminar las palabras que son Stop Words\n",
        "    5. Pasar el texto a minúsculas\n",
        "    6. Lematización\n",
        "    \n",
        "    \n",
        "* **Nota**: *la normalización de texto que se va a codificar a continuación puede codificarse de forma más optimizada sin la necesidad de recorrer tantas veces la lista de tokens. Ya que este es un ejemplo con fines didácticos, este se centra en los conceptos y no en la optimización*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6rLGFCGmI-s",
        "outputId": "b922beed-2f10-4918-8a73-d5a2bfbc17a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F_N7RZmgl8CD"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P3mQo0d1l8CE"
      },
      "outputs": [],
      "source": [
        "def get_tokens(text):\n",
        "    \"\"\"\n",
        "    Función que dado un texto devuelve una lista con las palabras del texto no vacias\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [word.text.strip() for word in doc if len(word.text.strip()) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R8BB9_g2l8CE"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(words):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras, elimina los signos de puntuación\n",
        "    \"\"\"\n",
        "    doc = spacy.tokens.doc.Doc(nlp.vocab, words=words)\n",
        "    return [word.text for word in doc if not word.is_punct]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IOnk8vRMl8CE"
      },
      "outputs": [],
      "source": [
        "def remove_short_words(words, num_chars):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras y un número mínimo de caracteres que tienen que tener\n",
        "    las palabras, elimina todas las palabras que tengan menos caracteres que los indicados\n",
        "    \"\"\"\n",
        "    return [word for word in words if len(word) > num_chars]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SNzapEndl8CE"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(words):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras, elimina las Stop Words\n",
        "    \"\"\"\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return [word.text for word in doc if not word.is_stop]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xxCkNZysl8CE"
      },
      "outputs": [],
      "source": [
        "def to_lowercase(words):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras, las transforma a minúsculas\n",
        "    \"\"\"\n",
        "    return [word.lower() for word in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dOPU6lZCl8CF"
      },
      "outputs": [],
      "source": [
        "def lemmatizer(words):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras, devuelve esa lista con el lema de cada una de esas palabras\n",
        "    \"\"\"\n",
        "    doc = nlp(\" \".join(words))\n",
        "    return [word.lemma_ for word in doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bVS4Y6kZl8CF"
      },
      "outputs": [],
      "source": [
        "def normalize(text):\n",
        "    \"\"\"\n",
        "    Dado un texto, devuelve el texto tokenizado y normalizado\n",
        "    \"\"\"\n",
        "    words = get_tokens(text=text)\n",
        "    words = remove_punctuation(words=words)\n",
        "    words = remove_short_words(words=words, num_chars=3)\n",
        "    words = remove_stop_words(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = lemmatizer(words)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kqQwpzRl8CF"
      },
      "source": [
        "#### Pasamos a tokenizar y normalizar el siguiente texto usando la función de normalización realizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HExO2FHtl8CF",
        "outputId": "db397210-057c-4137-c0e4-2f1430345772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fernando', 'alonso', 'vuelto', 'sacar', 'petróleo', 'carrera', 'salir', 'acabar', 'premio', 'coronado', 'adelantar', 'pista', 'sebastiar', 'vettel', 'líder', 'mundial', 'querido', 'sacar', 'pecho', 'coche', 'tocado', 'problema', 'dirección', 'claro', 'desventaja', 'perder', 'recta', 'imposible', 'adelantar él', 'conseguir', 'pillar él', 'aber', 'pensar', 'curvo', 'poder', 'intentar', 'salir', 'contento', 'séptimos', 'sumar', 'punto', 'carrera', 'señalado']\n"
          ]
        }
      ],
      "source": [
        "raw = \"\"\"Fernando Alonso ha vuelto a sacar petróleo de la carrera, saliendo 13º y acabando 7º un\n",
        "         gran premio que ha coronado adelantando en pista a Sebastian Vettel, líder del Mundial.\n",
        "         Aunque no ha querido sacar pecho por ello: \"Su coche estaba tocado, tenía problemas de dirección,\n",
        "         estaban en clara desventaja e iba perdiendo cada vez más, vi que en la recta iba a ser imposible\n",
        "         adelantarle incluso con el DRS no conseguía pillarle así que como se abría mucho pensé que en la\n",
        "         primera curva que pudiera lo intentaba por dentro y a la primera salió bien y creo que hay que\n",
        "         estar contentos, séptimos otra vez, sumando puntos en las tres carreras\", ha señalado.\"\"\"\n",
        "print(normalize(raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn0qn0pYl8CF"
      },
      "source": [
        "#### En este ejemplo podemos ver como reducimos las palabras (tokens) del texto original, quedandonos con lo importante y normalizado\n",
        "#### Pasamos de 128 tokens del texto original a 44 tokens tras la normalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZonvkDxkl8CF",
        "outputId": "7b3ffb53-a74b-4766-f7b2-0cd32bc52da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens del texto original: 128\n",
            "Número de tokens distintos del texto original: 91\n",
            "Número de tokens tras la normalización: 43\n",
            "Número de tokens distintos tras la normalización: 40\n"
          ]
        }
      ],
      "source": [
        "print('Número de tokens del texto original: ' + str(len(get_tokens(raw))))\n",
        "print('Número de tokens distintos del texto original: ' + str(len(set(get_tokens(raw)))))\n",
        "print('Número de tokens tras la normalización: ' + str(len(normalize(raw))))\n",
        "print('Número de tokens distintos tras la normalización: ' + str(len(set(normalize(raw)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc1-FDGhl8CG"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "# Bonus Track - Tratamiento de Strings\n",
        "\n",
        "\n",
        "## Codificación de Caracteres (Unicode)\n",
        "\n",
        "\n",
        "* Uno de los quebraderos de cabeza que se tiene a la hora de trabajar con python (sobre todo con python 2.X) es el tema de la ***codificación de los textos (Strings)***.\n",
        "\n",
        "\n",
        "* En un principio los ordenadores se diseñaron para utilizar el alfabeto ingles (que entre otras cosas no tiene ni acentos ni letras como la \"ñ\" para el Español) y por ese motivo se definió la codificación ***ASCII*** (***A***merican ***S***tandard ***C***ode for ***I***nformation ***I***nterchange) definido con 128 caracteres (7 bits para representar los 2<sup>7</sup> = 128 caracteres).\n",
        "\n",
        "\n",
        "<img src=\"./imgs/019_ASCII.png\" style=\"width: 500px;\"/>\n",
        "\n",
        "\n",
        "* Dado que en el resto de lenguas en el mundo hay muchos más caracteres, se definió una nueva codificación de caracteres denominada como ***UNICODE*** que representa alrededor de 110.000 caracteres.\n",
        "\n",
        "\n",
        "* Por tanto para poder trabajar con Strings (codificados de diferente manera) se debería hacer lo siguiente:\n",
        "\n",
        "    1. ¿Cual es la codificación de mi fichero original?\n",
        "    2. **Decode**: Paso el string de mi fichero a Unicode (cambio de codificación)\n",
        "    3. Realizo las operaciones que sean necesarias sobre los strings codificados en **Unicode**\n",
        "    4. **Encode**: Escribo de **Unicode** a otra **codificación** el string con el que he trabajado\n",
        "\n",
        "<img src=\"./imgs/008_Unicode_Decode_Encode.png\" style=\"width: 500px;\"/>\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "## Operaciones con Strings\n",
        "\n",
        "\n",
        "* Muchas veces tenemos que realizar operaciones de transforación sobre palabras o textos. A continuación se muestran algunas de las funciones más útiles para trabajar con strings:\n",
        "\n",
        "|Nombre Función|Funcionalidad|\n",
        "|---|---|\n",
        "|[s.find(t)](#M1)|index of first instance of string t inside s (-1 if not found)|\n",
        "|[s.rfind(t)](#M2)|index of last instance of string t inside s (-1 if not found)|\n",
        "|[s.index(t)](#M3)|like s.find(t) except it raises ValueError if not found|\n",
        "|[s.rindex(t)](#M4)|like s.rfind(t) except it raises ValueError if not found|\n",
        "|[s.join(text)](#M5)|combine the words of the text into a string using s as the glue|\n",
        "|[s.split(t)](#M6)|split s into a list wherever a t is found (whitespace by default)|\n",
        "|[s.splitlines()](#M7)|split s into a list of strings, one per line|\n",
        "|[s.lower()](#M8)|a lowercased version of the string s|\n",
        "|[s.upper()](#M9)|an uppercased version of the string s|\n",
        "|[s.title()](#M10)|a titlecased version of the string s|\n",
        "|[s.strip()](#M11)|a copy of s without leading or trailing whitespace|\n",
        "|[s.replace(t, u)](#M12)|replace instances of t with u inside s|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEscP15Pl8CG"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M1\">s.find(t)</a>\n",
        "\n",
        "\n",
        "* Encuentra la posición (indice) del string que se pasa como parámetro empezando a contar desde la izquierda.\n",
        "\n",
        "\n",
        "* Si no encuentra el string, devuelve valor -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSfAq6Yil8CG",
        "outputId": "3cafb511-c6c9-4d15-e515-57c21d57153b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "s = 'Ricardo Moya'\n",
        "s.find('Moya')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPL_v_5Zl8CG",
        "outputId": "70dc92f4-7a15-4a5e-9ad0-8e1af48b3e63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "s.find('a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTKfjmwyl8CG",
        "outputId": "5bb46e5f-632b-40e6-81f1-5afcff5acd2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "s.find('e')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em5NWfRNl8CG"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M2\">s.rfind(t)</a>\n",
        "\n",
        "\n",
        "* Hace lo mismo que \"s.find(t)\" pero empezando a contar desde la derecha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y90a4tUil8CG",
        "outputId": "23e50041-0c42-46f9-9a0c-3adeeac4cf7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "s.rfind('a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYTKjJssl8CG",
        "outputId": "17a521b5-c268-4def-ba92-c442a78af945"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "s.rfind('Moya')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dru3V7aEl8CG"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M3\">s.index(t)</a>\n",
        "\n",
        "\n",
        "* Hace lo mismo que \"s.find(t)\", con la única diferencia que devuelve un error (en vez de un -1) si no encuentra el string que se pasa como parámetro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOot3VXpl8CG",
        "outputId": "aadbe20a-9c2e-4144-9357-832414a2098d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "s.index('Moya')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jh8ypfyl8CG",
        "outputId": "6218f574-bb0b-406b-d67a-bc3bb20ac709"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "s.index('a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "dC8CoGo4l8CH",
        "outputId": "e45bfd77-7588-4a45-9541-8566e46f50ee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "substring not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8ac0fc017686>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'e'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Devuelve un error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: substring not found"
          ]
        }
      ],
      "source": [
        "s.index('e') # Devuelve un error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHE1yVt4l8CH"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M4\">s.rindex(t)\n",
        "\n",
        "\n",
        "* Hace lo mismo que \"s.index(t)\" pero empezando a contar desde la derecha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys9jF2KDl8CH",
        "outputId": "469259f2-0af0-48e8-82a0-c1c2ce87d8c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "s.rindex('a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jcheo3nl8CH",
        "outputId": "ddae6846-51b8-4bd5-df74-6d3952c13081"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "s.rfind('Moya')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWjt53SNl8CH"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M5\">separador.join(text)</a>\n",
        "\n",
        "\n",
        "* Une cada letra del string que se le pasa como parámetro con el separador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d2wowvv4l8CH",
        "outputId": "8ab1e329-c968-42bf-d711-be1489e0ea23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'R-i-c-a-r-d-o- -M-o-y-a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "'-'.join(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtW3uLmKl8CH"
      },
      "source": [
        "* Esta es una función muy utilizada para formar una cadena de texto con separador (por ejemplo un espacio en blanco) a partir de una lista o tupla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EQY4hLJol8CH",
        "outputId": "96acad60-5a91-4d61-d914-c19c36ae9d71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Un radar multa a Mariano Rajoy por caminar demasiado rapido'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "lista = [\"Un\", \"radar\", \"multa\", \"a\", \"Mariano\", \"Rajoy\", \"por\", \"caminar\", \"demasiado\", \"rapido\"]\n",
        "' '.join(lista)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKAiSwmsl8CH"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M6\">s.split(t)</a>\n",
        "\n",
        "\n",
        "* Divide el String \"***s***\" en una lista siempre que encuentre un separador \"***t***\".\n",
        "\n",
        "\n",
        "* Por defecto el separador es el espacio en blanco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2XjAwH6l8CH",
        "outputId": "a67f0fbc-b82d-4b68-fd89-9f96772e7961"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Un',\n",
              " 'radar',\n",
              " 'multa',\n",
              " 'a',\n",
              " 'Mariano',\n",
              " 'Rajoy',\n",
              " 'por',\n",
              " 'caminar',\n",
              " 'demasiado',\n",
              " 'rapido']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "texto = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
        "texto.split(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On-x5DfBl8CH"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M7\">s.splitlines()</a>\n",
        "\n",
        "\n",
        "* Divide un String \"***s***\" en una lista siempre que encuentre un salto de linea (\\n)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAfyemoMl8CH",
        "outputId": "088ff2df-2145-4f5d-d0f4-643ec488134d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' linea 1', 'linea 2', 'linea 3']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "texto = \"\"\" linea 1\\nlinea 2\n",
        "linea 3\n",
        "\"\"\"\n",
        "\n",
        "texto.splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Wp1IrFl8CH"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M8\">s.lower()</a>\n",
        "\n",
        "\n",
        "* Transforma un String \"***s***\" a minúsculas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_b6tbqsOl8CH",
        "outputId": "0020607f-dec1-4949-a087-ccfde5a04874"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'minusculas'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "s = \"MiNuSCuLaS\"\n",
        "s.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fdrVFfTl8CI"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M9\">s.upper()</a>\n",
        "\n",
        "\n",
        "* Transforma un String \"***s***\" a mayúsculas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w1OUkGtYl8CJ",
        "outputId": "9fa59734-affe-4bea-e7b9-4f1e4eda3c58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MAYUSCULAS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "s = \"mAyUscUlAs\"\n",
        "s.upper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gBC3rXIl8CJ"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M10\">s.title()</a>\n",
        "\n",
        "\n",
        "* Transforma el String \"***s***\" en formato título; es decir, pone la primera letra de cada palabra de String en mayúsculas y el resto en minúsculas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8QdImb4Rl8CJ",
        "outputId": "7dcd4810-e4eb-4593-a50b-d0cf1a3bd8db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ricardo Moya'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "s = \"rIcArdO mOyA\"\n",
        "s.title()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-llDFAXl8CK"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M11\">s.strip()</a>\n",
        "\n",
        "\n",
        "* Elimina los espacios en blanco y caracteres espaciales que hay tanto a la decrecha como a la izquierda del String \"***s***\".\n",
        "\n",
        "\n",
        "* Existen también las variantes de:\n",
        "    - s.rstrip(): Elimina los espacios en blanco y caracteres espaciales que hay a la derecha del string.\n",
        "    - s.lstrip(): Elimina los espacios en blanco y caracteres espaciales que hay a la izquierda del string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M8P6lFMLl8CK",
        "outputId": "abcf0122-6b43-4ac3-e2f0-9b326f372691"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ricardo Moya'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "s = \"   \\tRicardo Moya  \\t  \"\n",
        "s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hGi_bvTRl8CK",
        "outputId": "650721ad-f6cb-469d-a75b-de3d74be1836"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'   \\tRicardo Moya'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "s.rstrip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EXOlU4bPl8CK",
        "outputId": "a7de60a1-c294-4de1-d9d0-9647cfd78761"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ricardo Moya  \\t  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "s.lstrip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbrInqxXl8CK"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "### <a name=\"M12\">s.replace(t, u)</a>\n",
        "\n",
        "\n",
        "* Dado un String \"***s***\" sustituye cada aparición \"***t***\" por \"***u***\", pasandose \"***t***\" y \"***u***\" como parámetro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jEM6rQOgl8CK",
        "outputId": "e7d72b36-9c83-45e6-8e62-10d54ab44b3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Un radar multa a un tio por caminar demasiado rapido'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "s = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
        "s.replace(\"Mariano Rajoy\", \"un tio\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}