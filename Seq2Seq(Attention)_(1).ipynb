{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/NLP/blob/main/Seq2Seq(Attention)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "P-Wv85pWwh5J",
        "outputId": "26cf1fc4-3488-4d09-9339-2a1f5d6b6c03"
      },
      "source": [
        "# code by Tae Hwan Jung @graykode\n",
        "# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "\n",
        "def make_batch():\n",
        "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
        "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
        "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
        "\n",
        "    # make tensor\n",
        "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "\n",
        "        # Linear for attention\n",
        "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
        "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
        "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
        "\n",
        "        trained_attn = []\n",
        "        hidden = enc_hidden\n",
        "        n_step = len(dec_inputs)\n",
        "        model = torch.empty([n_step, 1, n_class])\n",
        "\n",
        "        for i in range(n_step):  # each time step\n",
        "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
        "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
        "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
        "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
        "\n",
        "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
        "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
        "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
        "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
        "\n",
        "        # make model shape [n_step, n_class]\n",
        "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
        "\n",
        "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
        "        n_step = len(enc_outputs)\n",
        "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
        "\n",
        "        for i in range(n_step):\n",
        "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
        "\n",
        "        # Normalize scores to weights in range 0 to 1\n",
        "        return F.softmax(attn_scores).view(1, 1, -1)\n",
        "\n",
        "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
        "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
        "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_step = 5 # number of cells(= number of Step)\n",
        "    n_hidden = 128 # number of hidden units in one cell\n",
        "\n",
        "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
        "\n",
        "    word_list = \" \".join(sentences).split()\n",
        "    word_list = list(set(word_list))\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "    n_class = len(word_dict)  # vocab list\n",
        "\n",
        "    # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, 1, n_hidden)\n",
        "\n",
        "    model = Attention()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(2000):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_batch, hidden, output_batch)\n",
        "\n",
        "        loss = criterion(output, target_batch.squeeze(0))\n",
        "        if (epoch + 1) % 400 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test\n",
        "    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
        "    test_batch = torch.FloatTensor(test_batch)\n",
        "    predict, trained_attn = model(input_batch, hidden, test_batch)\n",
        "    predict = predict.data.max(1, keepdim=True)[1]\n",
        "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
        "\n",
        "    # Show Attention\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(trained_attn, cmap='viridis')\n",
        "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
        "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    plt.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\n",
            "<ipython-input-1-77b3f64f3178>:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
            "<ipython-input-1-77b3f64f3178>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(attn_scores).view(1, 1, -1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0400 cost = 0.000506\n",
            "Epoch: 0800 cost = 0.000163\n",
            "Epoch: 1200 cost = 0.000080\n",
            "Epoch: 1600 cost = 0.000046\n",
            "Epoch: 2000 cost = 0.000030\n",
            "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-77b3f64f3178>:118: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
            "<ipython-input-1-77b3f64f3178>:119: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}