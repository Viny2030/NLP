{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/NLP/blob/main/Seq2Seq(Attention)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "q1LjrSiNx0p3",
        "outputId": "df574576-1d04-4250-9308-bb3cb2adfda3"
      },
      "source": [
        "# code by Tae Hwan Jung @graykode\n",
        "# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "\n",
        "def make_batch():\n",
        "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
        "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
        "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
        "\n",
        "    # make tensor\n",
        "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "\n",
        "        # Linear for attention\n",
        "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
        "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
        "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
        "\n",
        "        trained_attn = []\n",
        "        hidden = enc_hidden\n",
        "        n_step = len(dec_inputs)\n",
        "        model = torch.empty([n_step, 1, n_class])\n",
        "\n",
        "        for i in range(n_step):  # each time step\n",
        "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
        "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
        "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
        "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
        "\n",
        "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
        "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
        "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
        "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
        "\n",
        "        # make model shape [n_step, n_class]\n",
        "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
        "\n",
        "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
        "        n_step = len(enc_outputs)\n",
        "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
        "\n",
        "        for i in range(n_step):\n",
        "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
        "\n",
        "        # Normalize scores to weights in range 0 to 1\n",
        "        return F.softmax(attn_scores).view(1, 1, -1)\n",
        "\n",
        "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
        "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
        "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_step = 5 # number of cells(= number of Step)\n",
        "    n_hidden = 128 # number of hidden units in one cell\n",
        "\n",
        "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
        "\n",
        "    word_list = \" \".join(sentences).split()\n",
        "    word_list = list(set(word_list))\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "    n_class = len(word_dict)  # vocab list\n",
        "\n",
        "    # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, 1, n_hidden)\n",
        "\n",
        "    model = Attention()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(2000):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_batch, hidden, output_batch)\n",
        "\n",
        "        loss = criterion(output, target_batch.squeeze(0))\n",
        "        if (epoch + 1) % 400 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test\n",
        "    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
        "    test_batch = torch.FloatTensor(test_batch)\n",
        "    predict, trained_attn = model(input_batch, hidden, test_batch)\n",
        "    predict = predict.data.max(1, keepdim=True)[1]\n",
        "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
        "\n",
        "    # Show Attention\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(trained_attn, cmap='viridis')\n",
        "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
        "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    plt.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\n",
            "<ipython-input-1-77b3f64f3178>:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
            "<ipython-input-1-77b3f64f3178>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(attn_scores).view(1, 1, -1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0400 cost = 0.000494\n",
            "Epoch: 0800 cost = 0.000161\n",
            "Epoch: 1200 cost = 0.000080\n",
            "Epoch: 1600 cost = 0.000048\n",
            "Epoch: 2000 cost = 0.000031\n",
            "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-77b3f64f3178>:118: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
            "<ipython-input-1-77b3f64f3178>:119: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}