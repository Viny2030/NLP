{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/NLP/blob/main/22_Topic_Modeling_Tweets_Politica_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukHWPDicPI_-"
      },
      "source": [
        "# 22 - Latent Dirichlet Allocation - Ejemplo Topics & Terms en Tweets Políticos\n",
        "\n",
        "\n",
        "* En este Notebook vamos a agrupar los tweets publicados por los partidos políticos y los políticos para ver si verdaderamente el LDA es capaz de diferenciar las 5 tendencias políticas (5 Topics) que hay en estos tweets.\n",
        "\n",
        "\n",
        "* Por otro lado tambien es interesante ver cuales son las palabras (Terms) característicos de cada uno de estos Topics.\n",
        "\n",
        "\n",
        "* Para ello vamos a realizar los siguientes pasos:\n",
        "\n",
        "    1. Carga de datos\n",
        "    2. Normalizar los tweets (igual que en el notebook *13_PoC_Tendencias_Politicas_Twitter_Generacion_Exportacion_Modelos.ipynb*)\n",
        "    3. Creacción del diccionario y la bolsa de palabras\n",
        "    4. Selección del número óptimo de Topics\n",
        "    5. Creacción del Modelo\n",
        "    6. Visualización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAJ-zEkBPI__"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "## Carga de Datos\n",
        "\n",
        "\n",
        "* El fichero que contiene los tweets lo podemos leer como un '*csv*' con pandas pasandole como separador '***::::***'.\n",
        "\n",
        "\n",
        "* Este fichero esta estructurado de la siguiente manera\n",
        "    - **Cuenta**: Cuenta de twitter\n",
        "    - **Partido**: Partido político al que pertenece (ciudadanos, podemos, pp, psoe)\n",
        "    - **Timestamp**: Instante en el que se publicó el tweet\n",
        "    - **Tweet**: Tweet.\n",
        "    \n",
        "    \n",
        "* Leemos los datos y mostramos una muestra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6gA9yx6cUF2C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# Use raw.githubusercontent.com to get the direct download link for the zip file\n",
        "TWEETS_FILE = 'https://raw.githubusercontent.com/Viny2030/datasets/main/tweets_politica%20(1).zip'\n",
        "\n",
        "# Download the zip file content using requests\n",
        "response = requests.get(TWEETS_FILE)\n",
        "response.raise_for_status()  # Raise an exception if the download fails\n",
        "\n",
        "# Read the zip file content into a pandas DataFrame\n",
        "df = pd.read_csv(io.BytesIO(response.content), sep='::::', engine='python', compression='zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "HWgasCwIPwQ1",
        "outputId": "e5af8b91-d53c-4ef7-fe12-78716eefd3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de Tweets Cargados: 175309\n",
            "Número de Tweets a procesar: 54296\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 cuenta     partido     timestamp  \\\n",
              "161784          PODEMOS     podemos  1.643906e+09   \n",
              "147736  carrizosacarlos  ciudadanos  1.638643e+09   \n",
              "126135           noepmp     podemos  1.631868e+09   \n",
              "123252         ivanedlm         vox  1.631858e+09   \n",
              "140285     _JuanEspadas        psoe  1.636479e+09   \n",
              "137342     CiudadanosCs  ciudadanos  1.635414e+09   \n",
              "156460     _JuanEspadas        psoe  1.642282e+09   \n",
              "148281           vox_es         vox  1.639069e+09   \n",
              "161633           vox_es         vox  1.643881e+09   \n",
              "135065   hermanntertsch         vox  1.634699e+09   \n",
              "\n",
              "                                                    tweet  \n",
              "161784  Hoy dejamos atrás el modelo de precariedad del...  \n",
              "147736  Qué gentuza. Pidiendo el \"apartheid\" del niño....  \n",
              "126135  3 años, 3 subidas.Para esto llegamos a los gob...  \n",
              "123252  Cuatro casos aislados al día https://t.co/szkR...  \n",
              "140285  El gobierno andaluz tiene la oportunidad de de...  \n",
              "137342  ⚠️ La chapuza de estas cuentas es el reflejo d...  \n",
              "156460  La violencia en el deporte no tiene cabida. La...  \n",
              "148281  ‼️ Este viernes, @Santi_ABASCAL viaja a Brasil...  \n",
              "161633  \"Están matando a los ganaderos\".VOX no abandon...  \n",
              "135065  Si está “atónito” y “perplejo” el Tribunal Sup...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75b174e5-bcad-41d8-9e52-0ebdcf05abae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cuenta</th>\n",
              "      <th>partido</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>161784</th>\n",
              "      <td>PODEMOS</td>\n",
              "      <td>podemos</td>\n",
              "      <td>1.643906e+09</td>\n",
              "      <td>Hoy dejamos atrás el modelo de precariedad del...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147736</th>\n",
              "      <td>carrizosacarlos</td>\n",
              "      <td>ciudadanos</td>\n",
              "      <td>1.638643e+09</td>\n",
              "      <td>Qué gentuza. Pidiendo el \"apartheid\" del niño....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126135</th>\n",
              "      <td>noepmp</td>\n",
              "      <td>podemos</td>\n",
              "      <td>1.631868e+09</td>\n",
              "      <td>3 años, 3 subidas.Para esto llegamos a los gob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123252</th>\n",
              "      <td>ivanedlm</td>\n",
              "      <td>vox</td>\n",
              "      <td>1.631858e+09</td>\n",
              "      <td>Cuatro casos aislados al día https://t.co/szkR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140285</th>\n",
              "      <td>_JuanEspadas</td>\n",
              "      <td>psoe</td>\n",
              "      <td>1.636479e+09</td>\n",
              "      <td>El gobierno andaluz tiene la oportunidad de de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137342</th>\n",
              "      <td>CiudadanosCs</td>\n",
              "      <td>ciudadanos</td>\n",
              "      <td>1.635414e+09</td>\n",
              "      <td>⚠️ La chapuza de estas cuentas es el reflejo d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156460</th>\n",
              "      <td>_JuanEspadas</td>\n",
              "      <td>psoe</td>\n",
              "      <td>1.642282e+09</td>\n",
              "      <td>La violencia en el deporte no tiene cabida. La...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148281</th>\n",
              "      <td>vox_es</td>\n",
              "      <td>vox</td>\n",
              "      <td>1.639069e+09</td>\n",
              "      <td>‼️ Este viernes, @Santi_ABASCAL viaja a Brasil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161633</th>\n",
              "      <td>vox_es</td>\n",
              "      <td>vox</td>\n",
              "      <td>1.643881e+09</td>\n",
              "      <td>\"Están matando a los ganaderos\".VOX no abandon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135065</th>\n",
              "      <td>hermanntertsch</td>\n",
              "      <td>vox</td>\n",
              "      <td>1.634699e+09</td>\n",
              "      <td>Si está “atónito” y “perplejo” el Tribunal Sup...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75b174e5-bcad-41d8-9e52-0ebdcf05abae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-75b174e5-bcad-41d8-9e52-0ebdcf05abae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-75b174e5-bcad-41d8-9e52-0ebdcf05abae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-327c325e-9112-4183-957e-10f915af93f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-327c325e-9112-4183-957e-10f915af93f6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-327c325e-9112-4183-957e-10f915af93f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"cuenta\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"carrizosacarlos\",\n          \"CiudadanosCs\",\n          \"PODEMOS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partido\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"ciudadanos\",\n          \"psoe\",\n          \"podemos\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4524399.845110529,\n        \"min\": 1631857647.0,\n        \"max\": 1643906228.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1643881184.0,\n          1638643251.0,\n          1635413724.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"\\\"Est\\u00e1n matando a los ganaderos\\\".VOX no abandonar\\u00e1 a los ganaderos que sufren el ataque del lobo.El PP y el PSOE son enemigos del campo. https://t.co/MAKwf0w7RY\",\n          \"Qu\\u00e9 gentuza. Pidiendo el \\\"apartheid\\\" del ni\\u00f1o. El nacionalismo es veneno. Espero condena de los responsables educativos a estas acciones. Qu\\u00e9 verg\\u00fcenza que el Presidente S\\u00e1nchez pacte con las fuerzas pol\\u00edticas que est\\u00e1n haciendo esto en Catalu\\u00f1a. https://t.co/mzZlfnyNBr\",\n          \"\\u26a0\\ufe0f La chapuza de estas cuentas es el reflejo de tener dentro del Gobierno a un partido como Podemos.La derogaci\\u00f3n de la reforma laboral y la ley de vivienda intervencionista ser\\u00e1n otra losa para el progreso de Espa\\u00f1a.#Espa\\u00f1aNecesitaOtraCosa https://t.co/7nmDH4Sx5d\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "\n",
        "# The rest of your code remains the same...\n",
        "# Eliminamos los tweets que tengan algún valor a nulo\n",
        "df = df.dropna()\n",
        "print('Número de Tweets Cargados: {num}'.format(num=df.shape[0]))\n",
        "\n",
        "# Los ordenamos por fecha\n",
        "df = df.sort_values(by='timestamp')\n",
        "\n",
        "# Filtramos los tweets a partir de una fecha\n",
        "DATE = \"01/09/2021\"\n",
        "timestamp = time.mktime(datetime.datetime.strptime(DATE, \"%d/%m/%Y\").timetuple())\n",
        "df = df[df.timestamp >= timestamp]\n",
        "\n",
        "# Nos quedamos solo con el nombre del partido y el tweet\n",
        "tweets = [list(x) for x in df[['tweet', 'partido']].values]\n",
        "\n",
        "# Imprimimos el número de tweets a procesar\n",
        "print('Número de Tweets a procesar: {num}'.format(num=df.shape[0]))\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XleRnkbPJAB"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "## Normalización\n",
        "\n",
        "* Utilizamos ***spaCy*** para la tokenización y normalización.\n",
        "\n",
        "\n",
        "* Tras realizar un análisis del contenido de los tweets pasamos a realizar las siguientes acciones para ***normalizar*** los tweets:\n",
        "    1. Pasamos las frases a minúsculas.\n",
        "    2. Sustituimos los puntos por espacios ya que hay muchas palabras unidas por un punto\n",
        "    3. Quitamos la almuhadilla de los hashtags para considerarlos como palabras.\n",
        "    4. Eliminamos los signos de puntuación.\n",
        "    5. Eliminamos las palabras con menos de 3 caracteres.\n",
        "    6. Eliminamos las Stop-Words.\n",
        "    7. Eliminamos los enlaces(http) y las menciones (@)\n",
        "    8. Pasamos la palabra a su lema\n",
        "\n",
        "\n",
        "* Todos estos pasos los vamos a realizar en una misma función.\n",
        "\n",
        "\n",
        "* ***NOTA***: Se pueden realizar más acciones de normalización que las realizadas, como tratamiento de emoticonos, tratamiento especial de referencia a cuentas, hashtags, etc. Al tratarse de un ejemplo didáctica se ha realizado una normalización '*sencilla*'.\n",
        "\n",
        "#### CUIDADO - IMPORTANTE:\n",
        "\n",
        "* Dado que los procesos de normalización de textos son muy pesados y tardan mucho, se ha implementado despues de la normalización de los tweets, un proceso de guardado de los tweets ya normalizados. Por tanto:\n",
        "    - Si es la primera vez que se ejecuta este notebook, se puede ejecutar completo sabiendo que se guardarán en un fichero binario los tweets normalizados. Este guardado se realiza [AQUI](#Escritura).\n",
        "    - En caso de haberse ejecutado el proceso de normalización de tweets y haberse guardado este en un fichero binario, no será necesario ejecutar las dos siguientes celdas de código y bastaría con ejecutar la celda de código que lee el fichero binario con los tweets normalizados. Esto se hace en la siguiente [CELDA](#Lectura)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kzrCTIgUWVX",
        "outputId": "10cb729f-edc6-42b0-d977-3b40206f177e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQRMZblVPJAB",
        "outputId": "5b87f773-dc41-4127-d192-f0e5a281b34d",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54296/54296 [07:50<00:00, 115.51it/s]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "def normalize(tweets, min_words=3):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de tweets, normaliza los tweets y devuelve una lista con los tweets normalizados,\n",
        "    descartando aquellos tweets que tras la normalización tengan menos de \"min_words\" palabras en el tweet.\n",
        "\n",
        "    :param tweets:       Lista de Tweets con el tweet y la clase a la que pertenece\n",
        "    :param min_words:    Número minimo de palabras que tiene que tener un tweet tras la normalización\n",
        "    :return:             Lista de Tweets normalizados\n",
        "    \"\"\"\n",
        "    tweets_list = []\n",
        "    for tweet in tqdm(tweets):\n",
        "        # Tokenizamos el tweets realizando los puntos 1,2 y 3.\n",
        "        tw = nlp(tweet[0].lower().replace('.', ' ').replace('#', ' ').strip())\n",
        "\n",
        "        # Normalizamos Puntos 4,5,6,7 y 8\n",
        "        tw = ([word.lemma_ for word in tw if (not word.is_punct)\n",
        "               and (len(word.text) > 2) and (not word.is_stop)\n",
        "               and (not word.text.startswith('@'))\n",
        "               and (not word.text.startswith('http'))\n",
        "               and (not ':' in word.text)])\n",
        "\n",
        "        # Eliminamos los tweets que tras la normalización tengan menos de \"min_words\" palabras\n",
        "        if len(tw) >= min_words:\n",
        "            tweets_list.append(tw)\n",
        "    return tweets_list\n",
        "\n",
        "# Normalizamos las frases\n",
        "X_norm = normalize(tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6ralhSzPJAB"
      },
      "source": [
        "#### <a name=\"Escritura\">Guardado de los tweets normalizados en un fichero binario</a>\n",
        "\n",
        "* Se guarda una lista de listas, donde en cada una de las listas se tiene:\n",
        "    - [0]: El Tweet normalizado\n",
        "    - [1]: La clase a la que pertenece el Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rNGU8qNdXSJM"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Create the 'models' directory if it doesn't exist\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "filename = './models/22_normalized_tweets_lda.pickle'\n",
        "save_list = open(filename,\"wb\")\n",
        "pickle.dump(X_norm, save_list)\n",
        "save_list.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj6wn0c9PJAB"
      },
      "source": [
        "#### <a name=\"Lectura\">Lectura de los tweets normalizados de un fichero binario</a>\n",
        "\n",
        "* Lectura de una lista con la siguiente estructura:\n",
        "    - [0]: El Tweet normalizado\n",
        "    - [1]: La clase a la que pertenece el Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "viteJxIzPJAB"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "filename = './models/22_normalized_tweets_lda.pickle'\n",
        "X_norm = pickle.load(open(filename, 'rb'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHYh8SncPJAC"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "## Creamos el Diccionario y la Matriz (Bolsa de Palabras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLGUItCwPJAC",
        "outputId": "37fba73f-7262-4bb3-89c8-6c7fe9ad0131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diccionario:\n",
            "Dictionary<93608 unique tokens: ['aplaudar', 'arce', 'añez', 'cartel', 'celebrar']...>\n",
            "\n",
            "Primer Documento del Corpus:\n",
            "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from gensim import corpora\n",
        "from collections import defaultdict\n",
        "\n",
        "# Creamos el diccionario (vocabulario)\n",
        "frequency = defaultdict(int)\n",
        "for doc in X_norm:\n",
        "    for token in doc:\n",
        "        frequency[token] += 1\n",
        "\n",
        "documents = [[token for token in doc] for doc in X_norm]\n",
        "dictionary = corpora.Dictionary(documents)\n",
        "print('Diccionario:\\n{}'.format(dictionary))\n",
        "\n",
        "\n",
        "# Creamos la Bolsa de Palabras\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
        "print('\\nPrimer Documento del Corpus:\\n{}'.format(corpus[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D47_ycNEPJAC"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "## Selección del número óptimo de Topics\n",
        "\n",
        "\n",
        "* Veamos la coherencia para modelos desde 2 a 10 temas (Topics)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "XPhN34PEPJAC",
        "scrolled": true,
        "outputId": "ec7f9eb6-3484-4180-e5ed-c9615526e67a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 2/9 [09:28<33:10, 284.30s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8a020cdc6fc7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmax_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     lda_model = LdaModel(corpus=corpus,\n\u001b[0m\u001b[1;32m      9\u001b[0m                          \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                          \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             self.add_lifecycle_event(\n\u001b[1;32m    523\u001b[0m                 \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m   1018\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached the end of input; now waiting for all remaining jobs to finish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m                         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mother\u001b[0m  \u001b[0;31m# frees up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[0;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;31m# update self with the new blend; also keep track of how much did\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0;31m# the topics change through this update, to assess convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mprevious_Elogbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_Elogbeta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mPosterior\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \"\"\"\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "coherence = []\n",
        "min_topics = 2\n",
        "max_topics = 11\n",
        "for num_topics in tqdm(range(min_topics, max_topics, 1)):\n",
        "    lda_model = LdaModel(corpus=corpus,\n",
        "                         id2word=dictionary,\n",
        "                         num_topics=num_topics,\n",
        "                         random_state=0,\n",
        "                         chunksize=100,\n",
        "                         passes=10,\n",
        "                         alpha='auto',\n",
        "                         per_word_topics=True)\n",
        "    coherencemodel = CoherenceModel(model=lda_model, texts=X_norm, dictionary=dictionary, coherence='u_mass')\n",
        "    coherence.append(coherencemodel.get_coherence())\n",
        "\n",
        "index = [\"Num Topics: {num}\".format(num=num) for num in range(min_topics, max_topics, 1)]\n",
        "pd.DataFrame(coherence, index=index, columns=['Coherence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-FKNsX-PJAC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(min_topics, max_topics, 1), coherence)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OeMQDYwPJAC"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "## Creamos el Modelo:\n",
        "\n",
        "\n",
        "* Con la coherencia obtenida, podemos observar que ***4 y 7 Topics*** serie un buen número de temas a seleccionar, aunque \"lo esperado\" hubiese sido tener 5 Topics correspondientes a 5 tendencias políticas.\n",
        "\n",
        "\n",
        "* Vamos a crear el modelo con 5 Topics para ver si vemos diferencias entre las 5 tendencias políticas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rXZgfjOPJAC"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViNaUpDFPJAC"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "## Visualización 5 Topics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUTskB_-X9lI"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KODdO4cLPJAC"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB8f5Xv3PJAC"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "## Nube de etiquetas por Tópico\n",
        "\n",
        "\n",
        "* Utilizando el último modelo generado del LDA, vamos a asignar el tópico al que pertenece cada documento y vamos a generar una nube de palabras por tópico.\n",
        "\n",
        "\n",
        "* Para ello vamos a obtener el tópico más problable dado un documento, obteniendo las probabilidades de pertenencia al tópico dado por la función \"*get_document_topics()*\".\n",
        "\n",
        "\n",
        "* Una vez asignado el tópico al documento creamos la nube de palabras (etiquetas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu0q_aAmPJAD"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# Obtenemos el tópico más probable para cada documento del corpus\n",
        "document_topics = [[tup[1] for tup in lst].index(max([tup[1] for tup in lst])) for lst in lda_model[corpus]]\n",
        "\n",
        "\n",
        "# Procesamiento y pintado de las nubes de etiquetas\n",
        "plt.figure(figsize=(20, 15))\n",
        "pos = 1\n",
        "for topic in set(document_topics):\n",
        "    # Obtenemos las palabras por tópico\n",
        "    words = ' '.join([' '.join(X_norm[i]) for i, x in enumerate(document_topics) if x == topic])\n",
        "\n",
        "    # Pintamos las 5 nubes de etiquetas\n",
        "    plt.subplot(3, 2, pos)\n",
        "    wordcloud = WordCloud(max_font_size=80, max_words=100, background_color=\"white\").generate(words)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Palabras del tópico {}\".format(topic))\n",
        "    pos += 1\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}