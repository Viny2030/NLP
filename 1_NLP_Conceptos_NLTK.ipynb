{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VA8b_HRFlu8-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41D1sjIvlkxG"
      },
      "source": [
        "# Procesamiento del Lenguaje Natural, con Python\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Índice\n",
        "\n",
        "#### Tema 1: Introducción\n",
        "\n",
        "\n",
        "* ¿Qué es un Lenguaje?\n",
        "\n",
        "\n",
        "* ¿Qué es el Procesamiento del Lenguaje Natural?\n",
        "\n",
        "\n",
        "* Herramientas en Python para NLP:\n",
        "    - NLTK\n",
        "    - SpaCy\n",
        "    - Gensim\n",
        "    - Scikit\n",
        "    - TensorFlow-Keras\n",
        "\n",
        "\n",
        "#### Tema 2: NLP - Conceptos y Preprocesamiento de texto\n",
        "\n",
        "\n",
        "* Conceptos:\n",
        "\n",
        "    - Corpus\n",
        "    - Bag of Words (BoW)\n",
        "    - Tokenización\n",
        "    - N-Grammas\n",
        "    - Stemming\n",
        "    - Lematización\n",
        "    - Stop-Words\n",
        "    - Parts of Speech (PoS)\n",
        "    - Named Entity Recognition (NER)\n",
        "\n",
        "\n",
        "* Normalización de textos: Preprocesamiento\n",
        "\n",
        "\n",
        "#### Tema 3: Analisis Automático de texto subjetivo (Clasificación de textos)\n",
        "\n",
        "\n",
        "* Introducción: Clasificación de textos con Naive Bayes\n",
        "\n",
        "\n",
        "* Clasificación de textos: Algoritmos de aprendizaje para la clasificación\n",
        "\n",
        "\n",
        "* Clasificación de textos: Redes Neuronales\n",
        "\n",
        "\n",
        "#### Tema 4: Topic Modeling (Clustering)\n",
        "\n",
        "\n",
        "* LSI: Latent Semantic Index\n",
        "\n",
        "\n",
        "* LDA: Latent Dirichlet Allocation\n",
        "\n",
        "\n",
        "* Visialización: pyLDAvis\n",
        "\n",
        "\n",
        "#### Tema 5: Uso de modelos pre-entrenado en \"Hugging Face\" (Transformers)\n",
        "\n",
        "\n",
        "* Clasificación\n",
        "\n",
        "\n",
        "* Traducción\n",
        "\n",
        "\n",
        "* Resumenes de textos\n",
        "\n",
        "\n",
        "#### Tema 6: Introducción a GPT3\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "# Tema 1: Introducción\n",
        "\n",
        "\n",
        "## ¿Qué es un Lenguaje?\n",
        "\n",
        "\n",
        "* Un Lenguajes es un conjunto potencialmente infinito de oraciones y sentencias de palabras construidas mediante reglas gramaticales, foneticas y de significación que rigen el propio lenguaje.\n",
        "\n",
        "\n",
        "* Nos encontramos con 3 tipos de lenguajes:\n",
        "<span></span><br><br>\n",
        "    - ***Lenguaje Natural***: Lengua o idioma que nace espontáneamente de un grupo de hablantes por la necesidad de establecer comunicación verbal. Ejm: Ingles, Castellano, Frances, Italiano, etc.\n",
        "<span></span><br><br>\n",
        "    - ***Lenguaje Formal***: Lenguajes diseñados para un ámbito de aplicación concreto, que se definen de manera precisa y libre de ambigüedad. Ejm: Matemático, Lógico, Musical, Programación (C, Java, Python, R, Scala, etc.)\n",
        "<span></span><br><br>\n",
        "    - ***Lenguaje Artificial***: Lenguajes diseñados antes de ser usados por sus parlantes. Es una mezcla entre los lenguajes naturales y formales. Ejm: Klingon.\n",
        "\n",
        "\n",
        "\n",
        "## ¿Qué es el Procesamiento del Lenjuaje Natural?\n",
        "\n",
        "\n",
        "* El ***Procesamiento del Lenguaje Natural*** (NLP) es un campo que combina la ***Informática***, la ***Inteligencia Artificial*** y la ***Lingüística***; que tiene como objetivo, tratar la interacción entre los lenguajes humanos (lenguajes naturales) y los dispositivos informáticos.\n",
        "\n",
        "\n",
        "* El NLP abarca los siguientes campos:\n",
        "    - Recuperación de información\n",
        "    - Extracción y categorización de información\n",
        "    - Análisis automático de texto subjetivo (Análisis de sentimientos)\n",
        "    - Traducción automática\n",
        "    - Generación del lenguaje\n",
        "    - Questions & Answering (Chatbots)\n",
        "\n",
        "\n",
        "<img src=\"./imgs/000_nlp.png\" style=\"width: 700px;\"/>\n",
        "\n",
        "\n",
        "\n",
        "## De la *Lingüística* al *Procesamiento del Lenguaje Natural*\n",
        "\n",
        "### - Lingüística\n",
        "\n",
        "\n",
        "* La ***lingüística*** es el estudio científico del lenguaje, incluyendo su gramática, semántica y fonética.\n",
        "\n",
        "\n",
        "* En términos generales, un ***lingüista*** es cualquier persona que estudia un idioma.\n",
        "\n",
        "\n",
        "### - Lingüística computacional\n",
        "\n",
        "\n",
        "* La lingüística computacional es el estudio de la lingüística utilizando las herramientas de la informática.\n",
        "\n",
        "\n",
        "### - Procesamiento estadístico del lenguaje natural\n",
        "\n",
        "\n",
        "* La lingüística computacional también se conoce con el nombre de Procesamiento del Lenguaje Natural, para reflejar el enfoque más ingenieril o empírico de los métodos estadísticos aplicados a la Lingüistica.\n",
        "\n",
        "\n",
        "* El dominio estadístico del campo, lleva a menudo a que el ***NLP*** sea descrito como ***Procesamiento Estadístico del Lenguaje Natural***, para distanciarse (en la definición) de los métodos clásicos de la lingüística computacional.\n",
        "\n",
        "\n",
        "### - Procesamiento del Lenguaje natural\n",
        "\n",
        "\n",
        "* Campo que combina la ***Informática***, la ***Inteligencia Artificial*** y la ***Lingüística***; que tiene como objetivo, tratar la interacción entre los lenguajes humanos (lenguajes naturales) y los dispositivos informáticos.\n",
        "\n",
        "\n",
        "## Herramientas en Python para el NLP\n",
        "\n",
        "\n",
        "* Aunque existen bastante librería en Python destinadas al Procesamiento del Lenguaje Natural o a resolver determinadas partes del NLP, mostramos a continuación una serie de librería que vamos a utilizar en este curso:\n",
        "<span></span><br><br>\n",
        "    - ***NLTK*** (https://www.nltk.org/): Es una librería desarrollada por Steven Bird y Edward Loper para el NLP (principalmente en Inglés) que incorpora muchas funcionalidades como, corpus, recursos léxicos, algoritmos de aprendizaje para el NLP, etc.\n",
        "<span></span><br><br>\n",
        "    - ***SpaCy*** (https://spacy.io/): Es una librería para el NLP incorpora funcionalidades como Tokenización, Lematización, PoS, NER, etc. en varios idiomas. A diferencia de NLTK que tienen fines de caracter didáctico, SpaCy esta pensado para explotarlo en entornos productivos.\n",
        "<span></span><br><br>\n",
        "    - ***Gensim*** (https://radimrehurek.com/gensim): Es una librería desarrollada por el Checo Radim Řehůřek, Ph.D, que tiene implementadas; entre otras cosas, algoritmos como el LSI y el LDA para la detección de tópicos (Topic Modeling)\n",
        "<span></span><br><br>\n",
        "    - ***Scikit*** (https://scikit-learn.org/): Es una librería que tiene implementada multitud de algoritmos de aprendizaje (regresión, clasificación, cluterización, reducción de la dimensionalidad) y funcionalidades para trabajar con estos algoritmos.\n",
        "<span></span><br><br>\n",
        "    - ***TensorFlow-Keras*** (https://www.tensorflow.org/): Es una librería desarrollada por Google para trabajar con Redes Neuronales (MLP, CNN, RNN). La versión 2 de TensorFlow hace uso del API de Keras para un desarrollo más sencillo.\n",
        "    \n",
        "    \n",
        "## Bibliografía recomendada para NLP (en Inglés)\n",
        "\n",
        "\n",
        "1. **Natural Language Processing with Python** de Steven Bird, Ewan Klein y Edward Loper. O'Reilly Media. Julio 2009.\n",
        "\n",
        "2. **Applied Text Analysis with Python**, de Benjamin Bengfort, Rebecca Bilbro y Tony Ojeda. O'Reilly Media. Junio 2018.\n",
        "\n",
        "3. **Natural Language Processing Crash Course for Beginners**, de AI Publishing. Agosto 2020.\n",
        "\n",
        "4. **Transformers for Natural Language Processing**,  de Denis Rothman. Packt Publishing. Enero 2021.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vmbQd9EQhFZa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%"
        },
        "id": "Jwz0YW9rg0Ik"
      },
      "source": [
        "# 01 - Introducción a la librería NLTK\n",
        "\n",
        "\n",
        "* NLTK (https://www.nltk.org/) es una librería para Python, usada para el análisis y la manipulación del lenguaje natural.\n",
        "\n",
        "\n",
        "* Se instala bien con el gestor de paquetes \"pip\" o con \"conda\" en caso de utilizarlo. Para instalarlo con pip o conda se realiza de la siguiente manera respectivamente:\n",
        "\n",
        "```\n",
        ">> pip install nltk\n",
        ">> conda install nltk\n",
        "```\n",
        "\n",
        "<hr>\n",
        "\n",
        "## 1.- Instalación y descargar de las bases de datos\n",
        "\n",
        "* NLTK utiliza una serie de bases de datos léxicas para la manipulación del lenguaje natural.\n",
        "\n",
        "\n",
        "* También dispone de una una serie de corpus (colección de textos) que nos podemos descargar para \"jugar\" con ellos.\n",
        "\n",
        "\n",
        "* Para descargarnos las bases de datos y los corpus realizaremos lo siguiente:\n",
        "    1. Importar la librería nltk\n",
        "    2. llamar a al método \"download\"<sup>(*)</sup>\n",
        "    3. Aparecerá una ventana emergente para seleccionar todo lo que NLTK nos permite descargar\n",
        "    \n",
        "    \n",
        "###### (*): Si utilizas un MAC es posible que al ejecutar el método \"download()\" haga un logout de la sesión. Para evitarlo y para que se descargue todo el contenido, es necesario pasarle al método \"download\" como parámetros aquello que nos queramos descargar, en nuestro caso todo:\n",
        "\n",
        "```\n",
        ">> nltk.download('all')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "#nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFFmtBzsjwaW",
        "outputId": "1e9bb45f-aae4-4498-f19a-e7d7cdb84375"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> Q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s32Q4PG8jzCu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6DpqVaVg0In"
      },
      "source": [
        "4. Seleccionar todo aquello que queramos descargar.\n",
        "    1. Para empezar seleccionamos todo (all)\n",
        "    2. Pulsamos el boton de descargar\n",
        "    \n",
        "<img src=\"./imgs/001_nltk_download_db.png\" style=\"width: 500px;\"/>\n",
        "\n",
        "5. Una vez que ya tenemos todo descargado no aparecerá con fondo verde todo lo descargardo:\n",
        "\n",
        "<img src=\"./imgs/002_nltk_download_all.png\" style=\"width: 500px;\"/>\n",
        "\n",
        "6. Llegados a este punto ya tenemos descargado todos los corpus y bases de datos lexicas en el directorio que se indicar en la ventana emergente.\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4m-8tX1Vj3Lm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wd2R91qg0Io"
      },
      "source": [
        "## 2.- Corpus\n",
        "\n",
        "\n",
        "* Un ***Corpus*** (en Latín \"*Cuerpo*\") dentro del contexto del NLP se refiere a una colección de textos como puede ser un conjunto de artítulo periodísticos, libros, críticas, tweets, etc.\n",
        "\n",
        "\n",
        "* NLTK dispone de una serie de ***Corpus*** con los que poder trabajar y realizar pruebas.\n",
        "\n",
        "\n",
        "* Algunos de los ***corpus*** que pueden ser de interés didáctico son los siguientes:\n",
        "\n",
        "|Corpus|Content|\n",
        "|---|---|\n",
        "|Brown Corpus|15 genres, 1.15M words, tagged, categorized|\n",
        "|CESS Treebanks|1M words, tagged and parsed (Catalan, Spanish)|\n",
        "|Gutenberg (selections)|18 texts, 2M words|\n",
        "|Inaugural Address Corpus|U.S. Presidential Inaugural Addresses (1789–present)|\n",
        "|Movie Reviews|2k movie reviews with sentiment polarity classification|\n",
        "|Reuters Corpus|1.3M words, 10k news documents, categorized|\n",
        "|Stopwords Corpus|2,400 stopwords for 11 languages|\n",
        "|WordNet 3.0 (English)|145k synonym sets|\n",
        "\n",
        "\n",
        "* Para más información relativa a los corpus ir al siguiente enlace: http://www.nltk.org/howto/\n",
        "\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iwzMSnNXj6UX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg2kOPAxg0Io"
      },
      "source": [
        "### 2.1.- Manejo de Corpus (funcionalidades)\n",
        "\n",
        "Dentro de NLTK podemos encontrarnos diferentes tipos de Corpus que podrían clasificarse en:\n",
        "\n",
        "* Textos planos: como el corpus de *Gutenberg*\n",
        "* Textos categorizados: como el corpus de *Bronwn* (15 generos)\n",
        "* Textos multicategóricos: como el corpus de *Reuters* (1 documentos, varias categorias)\n",
        "* Textos temporales: como el corpus *Inaugural Address Corpus*, discursos presidenciales a lo largo de la historia\n",
        "\n",
        "Para el manejo de estos corpus NLTK nos ofrece las siguientes funciones:\n",
        "\n",
        "|Example|Description|\n",
        "|---|---|\n",
        "|fileids()|the files of the corpus|\n",
        "|fileids([categories])|the files of the corpus corresponding to these categories|\n",
        "|categories()|the categories of the corpus|\n",
        "|categories([fileids])|the categories of the corpus corresponding to these files|\n",
        "|raw()|the raw content of the corpus|\n",
        "|raw(fileids=[f1,f2,f3])|the raw content of the specified files|\n",
        "|raw(categories=[c1,c2])|the raw content of the specified categories|\n",
        "|words()|the words of the whole corpus|\n",
        "|words(fileids=[f1,f2,f3])|the words of the specified fileids|\n",
        "|words(categories=[c1,c2])|the words of the specified categories|\n",
        "|sents()|the sentences of the whole corpus|\n",
        "|sents(fileids=[f1,f2,f3])|the sentences of the specified fileids|\n",
        "|sents(categories=[c1,c2])|the sentences of the specified categories|\n",
        "|abspath(fileid)|the location of the given file on disk|\n",
        "|encoding(fileid)|the encoding of the file (if known)|\n",
        "|open(fileid)|open a stream for reading the given corpus file|\n",
        "|root|if the path to the root of locally installed corpus|\n",
        "|readme()|the contents of the README file of the corpus|\n",
        "\n",
        "### 2.1.1.- Ejemplo con el corpus de Gutenberg\n",
        "\n",
        "**1. ¿Que ficheros componen el corpus?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS0gEoDakDxA",
        "outputId": "a4b7f234-b942-4c44-d500-8760c718e29c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppIqwwppj8tm",
        "outputId": "b92ef8ed-b579-4a11-b69d-610f49a9b4ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TlBehQeukBZp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1UGEb8Ag0Io"
      },
      "source": [
        "**2. ¿Cual es el contenido del fichero \"blake-poems.txt\"?**\n",
        "\n",
        "(Por legibilidad mostramos solo los 300 primeros caracteres)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contenido = gutenberg.raw(\"blake-poems.txt\")\n",
        "contenido[:300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Eh8ncyRUkNDB",
        "outputId": "aafb3104-5320-4c19-b4fd-572bee0ad76a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[Poems by William Blake 1789]\\n\\n \\nSONGS OF INNOCENCE AND OF EXPERIENCE\\nand THE BOOK of THEL\\n\\n\\n SONGS OF INNOCENCE\\n \\n \\n INTRODUCTION\\n \\n Piping down the valleys wild,\\n   Piping songs of pleasant glee,\\n On a cloud I saw a child,\\n   And he laughing said to me:\\n \\n \"Pipe a song about a Lamb!\"\\n   So I piped'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F-Fd8nBvkPzZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGhoOCz1g0Ip"
      },
      "source": [
        "**3. De cada uno de los ficheros mostramos:**\n",
        "    - Número de caracteres\n",
        "    - Número de palabras\n",
        "    - Número de frases\n",
        "    - Número de palabras distintas que aparecen en el texto (primero las pasamos a minúsculas)\n",
        "    - Número de medio de caracteres por palabra\n",
        "    - Número medio de palabras por frase\n",
        "    - Diversidad léxica (número de palabras / palabras distintas del texto)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkMxeM9jkYO7",
        "outputId": "da335a03-fea1-488c-8804-a4dbd3866809"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file in gutenberg.fileids():\n",
        "    num_chars = len(gutenberg.raw(file))\n",
        "    num_words = len(gutenberg.words(file))\n",
        "    num_sents = len(gutenberg.sents(file))\n",
        "    num_words_distinct = len(set([w.lower() for w in gutenberg.words(file)]))\n",
        "    avg_chars_words = int(num_chars/num_words)\n",
        "    avg_words_sents = int(num_words/num_sents)\n",
        "    lexical_diversity = int(num_words/num_words_distinct)\n",
        "    print(\"{num_chars:<10} {num_words:<10} {num_sents:<10} {num_words_distinct:<10} {avg_chars_words:<10} {avg_words_sents:<10} {lexical_diversity:<10} {file:<10}\"\n",
        "          .format(num_chars=num_chars, num_words=num_words, num_sents=num_sents,\n",
        "                  num_words_distinct=num_words_distinct, avg_chars_words=avg_chars_words,\n",
        "                  avg_words_sents = avg_words_sents, lexical_diversity=lexical_diversity, file=file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhz60lCTkSwq",
        "outputId": "fc031906-71a6-40e3-b539-a89410bb7b01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "887071     192427     7752       7344       4          24         26         austen-emma.txt\n",
            "466292     98171      3747       5835       4          26         16         austen-persuasion.txt\n",
            "673022     141576     4999       6403       4          28         22         austen-sense.txt\n",
            "4332554    1010654    30103      12767      4          33         79         bible-kjv.txt\n",
            "38153      8354       438        1535       4          19         5          blake-poems.txt\n",
            "249439     55563      2863       3940       4          19         14         bryant-stories.txt\n",
            "84663      18963      1054       1559       4          17         12         burgess-busterbrown.txt\n",
            "144395     34110      1703       2636       4          20         12         carroll-alice.txt\n",
            "457450     96996      4779       8335       4          20         11         chesterton-ball.txt\n",
            "406629     86063      3806       7794       4          22         11         chesterton-brown.txt\n",
            "320525     69213      3742       6349       4          18         10         chesterton-thursday.txt\n",
            "935158     210663     10230      8447       4          20         24         edgeworth-parents.txt\n",
            "1242990    260819     10059      17231      4          25         15         melville-moby_dick.txt\n",
            "468220     96825      1851       9021       4          52         10         milton-paradise.txt\n",
            "112310     25833      2163       3032       4          11         8          shakespeare-caesar.txt\n",
            "162881     37360      3106       4716       4          12         7          shakespeare-hamlet.txt\n",
            "100351     23140      1907       3464       4          12         6          shakespeare-macbeth.txt\n",
            "711215     154883     4250       12452      4          36         12         whitman-leaves.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pt4GzxhYkVmh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1f5pc-jg0Ip"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### 2.1.2.- Ejemplo con el corpus de Brown\n",
        "\n",
        "Este es un corpus que contiene una serie de textos categorizados (o tageados) con un tipos de genero\n",
        "\n",
        "**1. ¿Cuales son las categorias del corpus de Brown?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb8O6qVKklpM",
        "outputId": "f57dd7fb-3767-4c10-c04c-2e4414c0f436"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5LRTB8Dkiz0",
        "outputId": "d6640f76-61f0-4f89-86eb-03225d9e15c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fQ1w3a24kqqF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bIItTzug0Iq"
      },
      "source": [
        "***2. ¿Qué ficheros componen la categoría de noticias (news)?*** (por legibilidad solo mostramos 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown.fileids(['news'])[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFsli0xaktGl",
        "outputId": "72cb72f7-0492-4949-c69f-2d21647d1934"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ca01', 'ca02', 'ca03', 'ca04', 'ca05']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aT-xmXGYkvel"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmxmDrxIg0Iq"
      },
      "source": [
        "***3.¿Que categorias corresponden al fichero \"ca01\"?***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown.categories(fileids=['ca01'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HD_qcoXkzRl",
        "outputId": "0810f1d4-c523-478e-82c9-79e74a3bd444"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['news']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pbx42pMXk1__"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGjEfNjIg0Iq"
      },
      "source": [
        "***4.¿Que palabras corresponden a la categoria humor?***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words(categories='humor')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wM6-z17k4hO",
        "outputId": "decc7a0c-f933-4371-8b00-76da29d6189f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It', 'was', 'among', 'these', 'that', 'Hinkle', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Er5warRzk752"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Prvtclg0Iq"
      },
      "source": [
        "<hr>\n",
        "\n",
        "## 3.- WordNet\n",
        "\n",
        "* ***WordNet*** es un diccionario semántico y jerárquico en Ingles compuesto por una 155k palabras y 117K sinónimos.\n",
        "\n",
        "\n",
        "* De forma jerarquica, esta estructurado de tal manera que hay una serie de palabras llamadas \"***unique beginers***\" o \"*root synsets*\" que son palabras que definen \"conceptos\" muy generales y a partir de esos conceptos generales engloban una serie de palabras pertenecientes a ese concepto. Veamos el siguiente ejemplo:\n",
        "\n",
        "<img src=\"./imgs/003_wordnet-hierarchy.png\" style=\"width: 500px;\"/>\n",
        "<p style=\"text-align: center;\">imagen obtenida del libro: \"<i>Natural Language Procesing with Python</i>\"</p>\n",
        "\n",
        "\n",
        "* En este ejemplo vemos como un \"*camión*\" (truck) esta definido como un \"*vehiculo motorizado*\" (motor vehicle) y este a su vez esta definido por otra palabra de nivel conceptual superior, hasta llegar a un muy alto nivel de palabra que lo define como un \"*artefacto*\" (artefact).\n",
        "\n",
        "\n",
        "* Este seria (\"a grandes rasgos\") como está organizado este diccionario, de tal manera que permite obtener de una palabra cosas como:\n",
        "    * Sinónimos\n",
        "    * Antónimos\n",
        "    * Hipérnimos\n",
        "    * Hipónimos\n",
        "    * Merónimos\n",
        "    * Holónimos\n",
        "    * Etc.\n",
        "    \n",
        "\n",
        "Veamos a continuación un ejemplo con la palabra \"motorcar\" y como nos daría una lista de sinónimos (synset) de esa palabra con la función \"synsets()\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfCk9Kf5lAu3",
        "outputId": "29ca4b5d-a8a0-42aa-9f42-3445a0faaa0c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "wn.synsets('motorcar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fuzdr4Sbk-O_",
        "outputId": "6f837ae4-02ba-49ba-d788-e94bac1eebb7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('car.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l39AV_sClG7n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYfvI0TPg0Ir"
      },
      "source": [
        "* En este caso nos devuelve una lista de sinónimos (synset), que serian los \"nodos\" del diccionario jerarquico a partir del cual se relaciona esa palabra. Para este ejemplo solo nos ha dado un sinónimo.\n",
        "\n",
        "* A partir del \"nodo\" 'car.n.01' vamos a:\n",
        "    * Obtener su definición\n",
        "    * Obtener los lemas de sus sinónimos (de la palabra car no de la palabra motorcar)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "definicion = wn.synset('car.n.01').definition()\n",
        "sinonimos = wn.synset('car.n.01').lemma_names()\n",
        "\n",
        "print('Definición: ' + definicion)\n",
        "print('Sinónimos: ' + str(sinonimos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUD84a9MlJ5Q",
        "outputId": "23b17de2-1b3e-4aa3-9a08-0f97ca3d24f7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definición: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
            "Sinónimos: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Py65-rgwlduc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeIvg13Cg0Ir"
      },
      "source": [
        "### Relación semántica entre palabras\n",
        "\n",
        "* Otro tema interesante que tenemos con ***WordNet*** es que nos permite ver la relación semáncia o la similaridad que hay entre palabras veamos por ejemplo la similaridad entre las siguientes palabras:\n",
        "\n",
        "    * car\n",
        "    * truck\n",
        "    * dog\n",
        "\n",
        "* Primero obtenemos alguno de los \"nodos\" de la palabra (el primero)\n",
        "* Comparamos la similaridad \"nodo\" con \"nodo\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "car = wn.synsets('car')[0]\n",
        "truck = wn.synsets('truck')[0]\n",
        "dog = wn.synsets('dog')[0]\n",
        "\n",
        "print('Similaridad entre Coche y Camión: ' + str(car.path_similarity(truck)))\n",
        "print('Similaridad entre Coche y Perro: ' + str(car.path_similarity(dog)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riBhohOQlgX9",
        "outputId": "7d8e699c-f2d2-4e73-baf7-e53326d6f730"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridad entre Coche y Camión: 0.3333333333333333\n",
            "Similaridad entre Coche y Perro: 0.07692307692307693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LeJUyCae628"
      },
      "source": [
        "# 02 - Conceptos para el Procesamiento del Lenguaje Natural (NLP)\n",
        "\n",
        "\n",
        "* En este Notebook vamos a enumerar y definir algunos de los conceptos más importantes que se dan en el Procesamiento del Lenguaje Natural y a ver algunos ejemplos en código.\n",
        "\n",
        "\n",
        "* Mostremos a continuación la definición de estos conceptos:\n",
        "\n",
        "\n",
        "### 1.- Corpus:\n",
        "\n",
        "* Un ***Corpus*** (en Latín \"cuerpo\") en el NLP se refiere a una colección de textos como pueda ser un conjunto de artículos periodísticos, libros, críticas, tweets, etc.\n",
        "\n",
        "### 2.- Bag of Words (BoW):\n",
        "\n",
        "* ***BoW*** (Bolsa de palabras) es un modelo que se utiliza para simplificar el contenido de un documento (o conjunto de documentos) omitiendo la gramática y el orden de las palabras, centrándose solo en el número de ocurrencias de palabras dentro del texto.\n",
        "\n",
        "### 3.- Normalización:\n",
        "\n",
        "* La ***normalización*** es una tarea que tiene como objetivo poner todo el texto en igualdad de condiciones:\n",
        "\n",
        "    - Convertir todo el texto en mayúscula o minúsculas\n",
        "    - Eliminar, puntos, comas, comillas, etc.\n",
        "    - Convertir los números a su equivalente a palabras\n",
        "    - Quitar palabras que no aportan significado al texto (Stop-words)\n",
        "    - Etc.\n",
        "\n",
        "### 4.- Tokenización:\n",
        "\n",
        "* Es una tarea que divide las cadenas de texto del documento en piezas más pequeñas o tokens. En la fase de tokenización los documentos se dividen en oraciones y estas se \"tokenizan\" en palabras. Aunque la tokenización es el proceso de dividir grandes cadenas de texto en cadenas más pequeñas, se suele diferenciar la:\n",
        "<span></span><br><br>\n",
        "    - ***Segmentación***: Tarea de dividir grandes cadenas de texto en piezas más pequeñas como oraciones o párrafos.\n",
        "<span></span><br><br>\n",
        "    - ***Tokenización***: Tarea de dividir grandes cadenas de texto solo y exclusivamente en palabras.\n",
        "\n",
        "\n",
        "### 5.- Stemming:\n",
        "\n",
        "* ***Stemming*** es el proceso de eliminar los afijos (sufijos, prefijos, infijos, circunflejos) de una palabra para obtener un tallo de palabra.\n",
        "<span></span><br><br>\n",
        "     + *Ejemplo*: Conduciendo -> conducir\n",
        "\n",
        "\n",
        "### 6.- Lematización:\n",
        "\n",
        "\n",
        "* La ***lematización*** es el proceso lingüístico que sustituye una palabra con forma flexionada (plurales, femeninos, verbos conjugados, etc.) por su lema; es decir, por una palabra válida en el idioma.\n",
        "\n",
        "\n",
        "* Si lo queremos definir de otra manera es sustituir una palabra con forma flexionada por la palabra que encontraríamos en el diccionario.\n",
        "<span></span><br><br>\n",
        "    + *Ejemplo*: Coches -> Coche; Guapas -> Guapo\n",
        "\n",
        "\n",
        "### 7.- Stop Words:\n",
        "\n",
        "\n",
        "* Son palabras que no aportan nada al significado de las frases como las preposiciones, determinantes, etc.\n",
        "\n",
        "\n",
        "### 8.- Parts-of-speech (POS) Tagging:\n",
        "\n",
        "\n",
        "* Consiste en asignar una etiqueta de categoría a las partes tokenizadas de una oración. El etiquetado POS más popular sería identificar palabras como sustantivos, verbos, adjetivos, etc.\n",
        "\n",
        "\n",
        "* En la lengua castellana nos podemos encontrar 9 categorías de palabras:\n",
        "\n",
        "    - Artículo o determinante\n",
        "    - Sustantivo o nombre\n",
        "    - Pronombre\n",
        "    - Verbo\n",
        "    - Adjetivo\n",
        "    - Adverbio\n",
        "    - Preposición\n",
        "    - Conjunción\n",
        "    - Interjección\n",
        "\n",
        "\n",
        "### 9.- n-grammas:\n",
        "\n",
        "\n",
        "* Los ***n-gramas*** son otro modelo de representación para simplificar los contenidos de selección de texto.\n",
        "\n",
        "\n",
        "* A diferencia de la representación sin orden de una bolsa de palabras (bag of words), el modelado de n-gramas está interesado en preservar secuencias contiguas de N elementos de la selección de texto.\n",
        "\n",
        "<hr>\n",
        "\n",
        "# Ejemplos con NLTK\n",
        "\n",
        "*NOTA: Los conceptos de \"Corpus\" (1), \"Bag of Words\" (2) y \"Normalización\" (3) son unos conceptos más amplios que explicar que el resto y por tanto se explicaran en otros notebooks de manera específica.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fMajoFIe62-"
      },
      "source": [
        "## 4.- Tokenización\n",
        "\n",
        "* Divide las cadenas de texto del documento en piezas más pequeñas o tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwFkJp0le62_",
        "outputId": "e79da2d9-95c0-433a-e595-1491a460bd90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Una', 'multa', 'de', 'transito', 'es', 'muy', 'onerosa', 'en', 'la', 'Ciudad', 'de', 'Buenos', 'Aires']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "doc = \"Una multa de transito es muy onerosa en la Ciudad de Buenos Aires\"\n",
        "words = nltk.word_tokenize(doc)\n",
        "print (words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZC9Tw7Ke63A"
      },
      "source": [
        "## 5.- Stemming\n",
        "\n",
        "* Proceso de eliminar los afijos\n",
        "\n",
        "\n",
        "* Para realizar el Stemming con NLTK tenemos que seleccionar el \"Stemmer\" adecuado dependiendo del idioma.\n",
        "\n",
        "\n",
        "* En NLTK existen dos \"Stemmers\" que son los siguientes:\n",
        "    * PorterStemmer\n",
        "    * SnowballStemmer\n",
        "\n",
        "\n",
        "* Para más información sobre estos ver el siguiente enlace: http://www.nltk.org/howto/stem.html\n",
        "<span></span><br><br>\n",
        "     + *Ejemplo en Inglés* con el *PorterStemmer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NeucnoK5e63B",
        "outputId": "2faafbc7-b4ab-48f9-98e6-f812d00ab2a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "minimum\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stm = PorterStemmer()\n",
        "print (stm.stem('running'))\n",
        "print (stm.stem('minimum'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5M-G-61e63B"
      },
      "source": [
        "* Los Stemmers de NLTK para idiomas distintos al Ingles son relativamente malos ya que NLTK esta pensado para la lengua inglesa.\n",
        "    + *Ejemplo en Español* con el *SnowballStemmer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hBuCFzjke63B",
        "outputId": "1860b29d-7a0d-4e7a-9065-74a2f55b99db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corr\n",
            "minim\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stm = SnowballStemmer('spanish') # Hay que indicarle explicitamente el idioma\n",
        "print (stm.stem('corriendo'))\n",
        "print (stm.stem('mínimo'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKWo-O3ce63C"
      },
      "source": [
        "## 6.- Lematización\n",
        "\n",
        "\n",
        "* Proceso lingüístico que sustituye una palabra con forma flexionada (plurales, femeninos, verbos conjugados, etc.) por su lema; es decir, por una palabra válida en el idioma.\n",
        "\n",
        "\n",
        "* La Lematización que hace NLTK solo es buena para la lengua inglesa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "B-xOVrpve63C",
        "outputId": "a7626d0c-9efb-47da-ecf7-6bd209cb8488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog\n",
            "perros\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemm = WordNetLemmatizer()\n",
        "print (lemm.lemmatize('dogs'))\n",
        "print (lemm.lemmatize('perros'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0iGX5tte63C"
      },
      "source": [
        "## 7.- Stop words\n",
        "\n",
        "\n",
        "* Son las palabras que no aportan nada al significado de la frase.\n",
        "\n",
        "\n",
        "* NLTK tiene para una serie de idiomas un listado de Stop Words.\n",
        "\n",
        "\n",
        "* Para el Español dispone de un listado de stop words:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4HG4W-snn4S",
        "outputId": "9076306f-aafe-4ba8-e433-254191f1851b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_SKZuQl0e63C",
        "outputId": "c19cc4f5-0fe9-47ec-f200-3c94eb2f9c9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hubiste', 'tendrá', 'mía', 'sentido', 'tened', 'tenidos', 'estabais', 'será', 'fuéramos', 'le', 'antes', 'una', 'teníamos', 'estéis', 'tendrían', 'tuviéramos', 'tengan', 'hubiesen', 'habidos', 'tuvo', 'tendrías', 'tuya', 'estaba', 'sentidas', 'tuyos', 'habrás', 'estas', 'eras', 'fueron', 'había', 'tendríais', 'seamos', 'habéis', 'estuviste', 'estés', 'otra', 'tendréis', 'tendríamos', 'las', 'vuestra', 'estos', 'sentida', 'habías', 'tú', 'hubieses', 'fueses', 'sobre', 'fuisteis', 'tenían', 'estarías', 'hubiéramos', 'fueran', 'habíais', 'estoy', 'fueras', 'teniendo', 'seré', 'está', 'que', 'tuviese', 'estuviera', 'suyos', 'tenga', 'sus', 'tenéis', 'hubiera', 'fuimos', 'todo', 'este', 'hubierais', 'lo', 'tenido', 'otros', 'estuvieseis', 'fuiste', 'seáis', 'estuvisteis', 'teníais', 'del', 'tengo', 'sentid', 'para', 'la', 'estad', 'eso', 'tuvisteis', 'ellos', 'habrías', 'hay', 'yo', 'fue', 'tendrán', 'tenemos', 'uno', 'ella', 'haya', 'es', 'nos', 'fuésemos', 'tuvimos', 'tendremos', 'estando', 'estuvieses', 'tenida', 'soy', 'tuvieran', 'hubieseis', 'estén', 'otro', 'seríamos', 'o', 'fui', 'ese', 'todos', 'un', 'en', 'tuvieses', 'estamos', 'habrían', 'otras', 'serían', 'al', 'su', 'se', 'cual', 'muy', 'ha', 'por', 'estaréis', 'habríamos', 'tengáis', 'porque', 'suya', 'hubo', 'tenías', 'durante', 'serán', 'unos', 'nosotras', 'habréis', 'nuestra', 'mis', 'tendría', 'habidas', 'fuerais', 'cuando', 'estadas', 'vosotros', 'nuestros', 'tuyo', 'seríais', 'mío', 'sin', 'tuviera', 'hayáis', 'estuvieras', 'estuviésemos', 'estaban', 'tenía', 'ni', 'tus', 'vuestras', 'fuera', 'también', 'hemos', 'habría', 'sí', 'sentidos', 'estuviesen', 'habido', 'estuvo', 'estados', 'hasta', 'estado', 'sintiendo', 'y', 'nada', 'estaríais', 'algunos', 'tuve', 'te', 'suyas', 'de', 'él', 'tuviste', 'habríais', 'habida', 'tuviésemos', 'hubieron', 'tuviesen', 'tuvieseis', 'estuve', 'habremos', 'estaría', 'estuviese', 'algo', 'quienes', 'ti', 'donde', 'estuvieran', 'estás', 'estarás', 'serás', 'habiendo', 'siente', 'mi', 'tuvieron', 'contra', 'estemos', 'eres', 'esto', 'nuestro', 'nosotros', 'estarían', 'habían', 'estaríamos', 'tuvieras', 'estáis', 'he', 'algunas', 'hubiésemos', 'me', 'hube', 'seremos', 'seas', 'eran', 'a', 'serías', 'estábamos', 'mí', 'quien', 'hubieran', 'tuyas', 'estarán', 'tengamos', 'ellas', 'habíamos', 'estuviéramos', 'nuestras', 'tu', 'tienes', 'sea', 'tendré', 'míos', 'sería', 'fuesen', 'esa', 'habré', 'estará', 'hubiese', 'hubieras', 'vuestros', 'más', 'pero', 'hayan', 'mucho', 'mías', 'seréis', 'has', 'les', 'entre', 'son', 'el', 'sean', 'estabas', 'tendrás', 'era', 'tanto', 'desde', 'con', 'como', 'hayamos', 'erais', 'no', 'fuese', 'han', 'sois', 'estuvierais', 'estuvimos', 'tenidas', 'los', 'esté', 'estaré', 'ya', 'esas', 'tengas', 'vosotras', 'éramos', 'vuestro', 'tiene', 'estuvieron', 'estada', 'esta', 'esos', 'muchos', 'estar', 'hubisteis', 'poco', 'están', 'hubimos', 'tienen', 'ante', 'e', 'habrán', 'hayas', 'habrá', 'os', 'somos', 'qué', 'estaremos', 'suyo', 'fueseis', 'tuvierais'}\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(set(stopwords.words('spanish')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvenLiEOe63C"
      },
      "source": [
        "* Este listado de palabras se utiliza para eliminarlas de los textos.\n",
        "\n",
        "\n",
        "* Veamos a continuación como obtener las stop words de una frase tras su tokenización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RQgHCb4re63C",
        "outputId": "e74138b9-e8ff-40c2-9a7c-ee54f3b7e198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en\n",
            "la\n",
            "de\n",
            "son\n"
          ]
        }
      ],
      "source": [
        "doc = \"Las multas en la Ciudad de Buenos Aires son onerosas\"\n",
        "words = nltk.word_tokenize(doc)\n",
        "for word in words:\n",
        "        if word in stopwords.words('spanish'):\n",
        "            print (word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9pL3Ocje63D"
      },
      "source": [
        "## 8.- Part of Speech (PoS)\n",
        "\n",
        "\n",
        "* Consiste en asignar una etiqueta de categoría a las partes tokenizadas de una oración: sustantivos, verbos, adjetivos, etc.\n",
        "\n",
        "\n",
        "* El PoS de NLTK solo esta disponible para el ingles y tiene las siguientes categorias:\n",
        "\n",
        "|Tag|Meaning|\n",
        "|---|---|\n",
        "|ADJ|adjective|\n",
        "|ADP|adposition|\n",
        "|ADV|adverb|\n",
        "|CONJ|conjunction|\n",
        "|DET|determiner|\n",
        "|NOUN|noun|\n",
        "|NUM|numeral|\n",
        "|PRT|particle|\n",
        "|PRON|pronoun|\n",
        "|VERB|verb|\n",
        "|.|punctuation|\n",
        "|X|other|\n",
        "\n",
        "\n",
        "* Nota: La tabla anterior no significa que solo asigne esas categorias, si no que tiene esas categorias y luego las va desgranando; por ejemplo, los verbos o adjetivos pueden ser de diferentes tipos y les pondrá una etiqueta en función de ese tipo.\n",
        "\n",
        "\n",
        "* Veamos a continuación un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH0kI1vzn2gD",
        "outputId": "cfeafb07-1f15-46d2-ea4a-63f8efd513ee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AhI0qkMCe63D",
        "outputId": "cdfbc714-6264-40f4-b725-514e6fb6fc27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Is', 'VBZ'), ('marathon', 'JJ'), ('running', 'VBG'), ('bad', 'JJ'), ('for', 'IN'), ('you', 'PRP'), ('?', '.')]\n"
          ]
        }
      ],
      "source": [
        "doc = nltk.word_tokenize('Is marathon running bad for you?')\n",
        "print (nltk.pos_tag(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnlNwrLDe63D"
      },
      "source": [
        "### PoS en Español:\n",
        "\n",
        "\n",
        "* Para poder \"tagear\" correctamente las palabras en Español, tenemos que descargarnos un diccionario específico para esta lengua.\n",
        "\n",
        "\n",
        "* El grupo de Procesamiento de Lenguaje Natural de la Universidad de Stanford ha desarrollado un diccionario en castellano que nos pertime etiquetar las palabras.\n",
        "\n",
        "\n",
        "* En el siguiente enlace se puede ver su descripción: https://nlp.stanford.edu/software/spanish-faq.shtml\n",
        "\n",
        "\n",
        "* Para ello debemos de descargarnos el software especifico proporcionado por la universidad de Standford a través del siguiente enlace: https://nlp.stanford.edu/software/tagger.shtml\n",
        "\n",
        "<img src=\"./imgs/004_Standford_tagger.png\" style=\"width: 600px;\"/>\n",
        "\n",
        "\n",
        "* Una vez descargada la librería tenemos que:\n",
        "    1. Descomprimir el fichero\n",
        "    2. Obtener el jar: stanford-postagger-3.9.2.jar\n",
        "    3. Obtener el tagger spanish.tagger que se encuentra dentro de la carpeta models\n",
        "\n",
        "\n",
        "* Estos ficheros necesarios ya estan copiados dentro del proyecto en la carpeta 'libs'\n",
        "\n",
        "\n",
        "* Veamos como ejecutarlo (*Nota: si se utiliza windows hay que poner las rutas absolutas de estos ficheros (variables 'jar' y 'tagger_file')*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "JTziJjGge63D",
        "outputId": "01bf4172-7824-4dd2-c632-e60f45cd2764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "Could not find stanford-postagger.jar jar file at ./libs/Standford_tagger/stanford-postagger-3.9.2.jar",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-5386f0c90b86>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtagger_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./libs/Standford_tagger/spanish.tagger\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;34m\"StanfordPOSTagger or StanfordNERTagger?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             )\n\u001b[0;32m---> 70\u001b[0;31m         self._stanford_jar = find_jar(\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m ):\n\u001b[0;32m--> 833\u001b[0;31m     return next(\n\u001b[0m\u001b[1;32m    834\u001b[0m         find_jar_iter(\n\u001b[1;32m    835\u001b[0m             \u001b[0mname_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             raise LookupError(\n\u001b[0m\u001b[1;32m    720\u001b[0m                 \u001b[0;34mf\"Could not find {name_pattern} jar file at {path_to_jar}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             )\n",
            "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-postagger.jar jar file at ./libs/Standford_tagger/stanford-postagger-3.9.2.jar"
          ]
        }
      ],
      "source": [
        "from nltk.internals import find_jars_within_path\n",
        "from nltk.tag import StanfordPOSTagger\n",
        "\n",
        "jar = \"./libs/Standford_tagger/stanford-postagger-3.9.2.jar\"\n",
        "tagger_file = \"./libs/Standford_tagger/spanish.tagger\"\n",
        "\n",
        "tagger = StanfordPOSTagger(tagger_file, jar)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"Las multas en la Ciudad de Buenos Aires son onerosas\"\n",
        "words = nltk.word_tokenize(doc)\n",
        "tags = tagger.tag(words)\n",
        "print(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "AMQxG9uMo2G0",
        "outputId": "999acbcb-a6bc-4ba2-85a6-78f43ece874c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tagger' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-1b52f06aba3d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Las multas en la Ciudad de Buenos Aires son onerosas\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tagger' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MKP16GZe63D"
      },
      "source": [
        "* En este caso el \"taggeo\" es diferente al de NLTK.\n",
        "\n",
        "\n",
        "* Si nos fijamos en la documentación:\n",
        "    - ('Un', 'di0000') -> Article (indefinite)\n",
        "    - ('radar', 'nc0s000') -> Common noun (singular)\n",
        "    - ('multa', 'nc0s000') -> Common noun (singular)\n",
        "    - ('a', 'sp000') -> Preposition\n",
        "    - ('Mariano', 'np00000') -> Proper noun\n",
        "    - ('Rajoy', 'np00000') -> Proper noun\n",
        "    - ('por', 'sp000') -> Preposition\n",
        "    - ('caminar', 'vmn0000') -> Verb (main, infinitive)\n",
        "    - ('demasiado', 'rg') -> Adverb (general)\n",
        "    - ('rapido', 'aq0000')  -> Adjective (descriptive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQIJF_mse63D"
      },
      "source": [
        "## 9.- n-grams\n",
        "\n",
        "* Modelo de representación que selecciona secuencias contiguas de N elementos de la selección de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hvFsGwjYe63D",
        "outputId": "77e7b312-ecdc-4c6e-b4af-f3244f24c5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Las', 'multas', 'en')\n",
            "('multas', 'en', 'la')\n",
            "('en', 'la', 'Ciudad')\n",
            "('la', 'Ciudad', 'de')\n",
            "('Ciudad', 'de', 'Buenos')\n",
            "('de', 'Buenos', 'Aires')\n",
            "('Buenos', 'Aires', 'son')\n",
            "('Aires', 'son', 'onerosas')\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "doc = \"Las multas en la Ciudad de Buenos Aires son onerosas\"\n",
        "words = nltk.word_tokenize(doc)\n",
        "num_elementos = 3\n",
        "n_grams = ngrams(words, num_elementos)\n",
        "for grams in n_grams:\n",
        "    print (grams)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}