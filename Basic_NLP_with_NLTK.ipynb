{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3",
      "name": "python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/NLP/blob/main/Basic_NLP_with_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'patching-pizzas:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4716%2F7197%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240615%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240615T000517Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5fe3ae45157cd651652cfc2f3d55d06886f2293a9872907c792d4aac99eecd5fdb9b016257f0e7e1645f8acc0f9f7105c61a2d5d66131d4645cbdb7f2caaf5db7902ffc132d61d74b9bf36cca3cb4a042ad029ac836986503d4c26ca299b32254593465c5658a4b9743b0a2c10a3db3a11a180ab1be32dc05fe003e25e6b04f77305b8f40c0174b6cd4cf08e0cf088741e75b13d15d8deb79d13e2ae9c4b8626e7bccf18940eff3c3e7a4dd382f29e0ce8533d70c92569ecc0347f89c9abe40dc8414daf9db77e5778b04d8caaaf4a2c6067a43d9607b63091c160a3ff90543a2fe8e2d34dff95d91ab0059a360a3d4c287d5d55b2b5ed859bc5bfeaf4c96310'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "MSdwfyFJNoFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd71285-de5b-4dc3-85f0-65d840adf5b6"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/4716/7197/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240615%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240615T000517Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=5fe3ae45157cd651652cfc2f3d55d06886f2293a9872907c792d4aac99eecd5fdb9b016257f0e7e1645f8acc0f9f7105c61a2d5d66131d4645cbdb7f2caaf5db7902ffc132d61d74b9bf36cca3cb4a042ad029ac836986503d4c26ca299b32254593465c5658a4b9743b0a2c10a3db3a11a180ab1be32dc05fe003e25e6b04f77305b8f40c0174b6cd4cf08e0cf088741e75b13d15d8deb79d13e2ae9c4b8626e7bccf18940eff3c3e7a4dd382f29e0ce8533d70c92569ecc0347f89c9abe40dc8414daf9db77e5778b04d8caaaf4a2c6067a43d9607b63091c160a3ff90543a2fe8e2d34dff95d91ab0059a360a3d4c287d5d55b2b5ed859bc5bfeaf4c96310 to path /kaggle/input/patching-pizzas\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "4c26dad8c106fb2eacc6d703993b524774467445",
        "_cell_guid": "258f18fc-e0b2-4a55-8927-51c46077017b",
        "id": "WoUXeRMjNoFg"
      },
      "cell_type": "markdown",
      "source": [
        "Introduction\n",
        "====\n",
        "**Natural Language Processing ** (NLP) is the task of making computers understand and produce human languages.\n",
        "\n",
        "And it always starts with the **corpus** i.e. *a body of text*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introducción**\n",
        "* El procesamiento del lenguaje natural * (PNL) es la tarea de hacer que las computadoras entiendan y produzcan idiomas humanos.\n",
        "\n",
        "Y siempre comienza con el corpus, es decir, un cuerpo de texto."
      ],
      "metadata": {
        "id": "30LD0xzhynQb"
      }
    },
    {
      "metadata": {
        "_uuid": "93f8697704b4f7c9a900bd26c341862d1ef82f05",
        "_cell_guid": "2762e23e-138d-4086-af46-53aa1d7d0bcd",
        "id": "PIOumndQNoFh"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "What is a Corpus?\n",
        "====\n",
        "\n",
        "There are many corpora (*plural of corpus*) available in NLTK, lets start with an English one call the **Brown corpus**.\n",
        "\n",
        "When using a new corpus in NLTK for the first time, downloads the corpus with the `nltk.download()` function, e.g.\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **¿Qué es un corpus?**\n",
        "Hay muchos corpus (plural de corpus) disponibles en NLTK, comencemos con un inglés en inglés.\n",
        "\n",
        "Cuando use un nuevo corpus en NLTK por primera vez, descarga el corpus con la función nltk.download (), p."
      ],
      "metadata": {
        "id": "wFCW20UmytlE"
      }
    },
    {
      "metadata": {
        "_uuid": "6a663051c176297596bc3174d10a1f8599247621",
        "_cell_guid": "bd910c35-dc49-4c07-9441-9d16d175580a",
        "id": "9v6Mxn4eNoFi"
      },
      "cell_type": "markdown",
      "source": [
        "After its downloaded, you can import it as such:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOHTtT2Fyxh_",
        "outputId": "198d7474-9a3a-4d22-c7d1-00ce1c610ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "637fad23e6bdfd6b8188a7c56318c91b6d3227e2",
        "_cell_guid": "2f1d4c2c-ea22-4494-a650-cfddd3aba49c",
        "collapsed": true,
        "id": "fqkf1-PrNoFi"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk.corpus import brown"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "_uuid": "14c6fc8bb6bdffbf6fb397c9d2c5da53d12c1e69",
        "_cell_guid": "121fd272-19fd-498f-a3e9-5295d09a2e16",
        "collapsed": true,
        "id": "Ji0QjSFNNoFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc72595-453f-4b32-88b8-52c5363fc8f9"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "brown.words() # Returns a list of strings"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "metadata": {
        "_uuid": "2841227d2eeba7a7629ffa690647c43dc07bea7f",
        "_cell_guid": "ebf89848-9442-4790-9575-a7f87234eebd",
        "collapsed": true,
        "id": "3nz3HxMXNoFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a9ed6b4-56d4-4f60-d5b8-15ade5293dc8"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "len(brown.words()) # No. of words in the corpus"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "metadata": {
        "_uuid": "2de0781902fe8662d73bda9381cab9e318cdebf8",
        "_cell_guid": "c3ba66d4-3e26-40fc-94d3-211581fa4c5c",
        "collapsed": true,
        "id": "C7arcj2xNoFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db6b3ca-418a-4cb8-dae9-eebf901ae75f"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "brown.sents() # Returns a list of list of strings"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {
        "_uuid": "a17b7f140a86b0541acf2796b9146955acd64201",
        "_cell_guid": "68725143-131d-4886-851c-f932c84588aa",
        "collapsed": true,
        "id": "sg3sQWdaNoFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4880a4-8808-4756-b177-b942d8374f36"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "brown.sents(fileids='ca01') # You can access a specific file with `fileids` argument."
      ]
    },
    {
      "metadata": {
        "_uuid": "4e2791d121eb8162f2031df6c126bcaa40d812d7",
        "_cell_guid": "972700b2-ee0b-4038-ba23-573287d08fd0",
        "id": "k4dIL6MLNoFj"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Fast Facts:**\n",
        "\n",
        "> The Brown Corpus of Standard American English was the first of the modern, computer readable, general corpora. It was compiled by W.N. Francis and H. Kucera, Brown University, Providence, RI. The corpus consists of one million words of American English texts printed in 1961.\n",
        "\n",
        "(Source: [University of Essex Corpus Linguistics site](  https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html))\n",
        "\n",
        ">  This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on ... (for a complete list, see http://icame.uib.no/brown/bcm-los.html).\n",
        "\n",
        "![](http://)(Source: [NLTK book, Chapter 2.1.3](http://www.nltk.org/book/ch02.html))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El Corpus Brown del Inglés Estadounidense Standard fue el primero de los corpuses modernos, legibles y generales. Fue compilado por la ONM Francis y H. Kucera, Brown University, Providence, RI. El corpus consta de un millón de palabras de textos en inglés americano impresos en 1961.\n",
        "\n",
        "(Fuente: Sitio de lingüística del Corpus de la Universidad de Essex)\n",
        "\n",
        "Este corpus contiene texto de 500 fuentes, y las fuentes han sido clasificadas por género, como noticias, editoriales, etc. (para una lista completa, consulte http://icame.uib.no/brown/bcm- Los.html).\n",
        "\n",
        "(Fuente: Libro NLTK, Capítulo 2.1.3)"
      ],
      "metadata": {
        "id": "wM0rcw4yzpJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://varieng.helsinki.fi/CoRD/corpora/BROWN/versions.html\n"
      ],
      "metadata": {
        "id": "9feyRPp10Akb"
      }
    },
    {
      "metadata": {
        "_uuid": "abcb302ff5d44499870e06f7f86490cf077fea2b",
        "_cell_guid": "1ecd0a80-5a12-444a-b9da-768ffef5133f",
        "id": "0zrI-RFTNoFj"
      },
      "cell_type": "markdown",
      "source": [
        "The actual `brown` corpus data is **packaged as raw text files**.  And you can find their IDs with:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIBRO = https://www.nltk.org/book/\n"
      ],
      "metadata": {
        "id": "cnllqYCg0ZeN"
      }
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {
        "_uuid": "8c979bdf698b8f7975b1216083168de0317c8ac5",
        "_cell_guid": "9bb09e9a-574f-4c3d-97eb-2091ce989cab",
        "collapsed": true,
        "id": "GgszxzAONoFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8fee19-506b-47b7-db10-2b0a3c4b6750"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "len(brown.fileids()) # 500 sources, each file is a source."
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12']\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "30e85f007a8812090f813d8212e43a834be3de29",
        "_cell_guid": "fca762b7-5403-446f-82ca-e91e4cbd51dd",
        "collapsed": true,
        "id": "r4m60wa-NoFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671d3b6f-7a95-4022-adb5-d61d8c430575"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(brown.fileids()[:100]) # First 100 sources."
      ]
    },
    {
      "metadata": {
        "_uuid": "b6572ea60d0142090d77a44fabc98ee29943a4bb",
        "_cell_guid": "74143a0e-40e2-4450-9ab4-427686f6ac87",
        "id": "QRyZmSnwNoFj"
      },
      "cell_type": "markdown",
      "source": [
        "You can access the raw files with:"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assembly/nn-hl session/nn-hl brought/vbd-hl much/ap-hl good/nn-hl \n",
            "The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
            "It/pps was/bedz faced/vbn immediately/rb with/in a/at showdown/nn on/in the/at schools/nns ,/, an/at issue/nn which/wdt was/bedz met/vbn squarely/rb in/in conjunction/nn with/in the/at governor/nn with/in a/at decision/nn not/* to/to risk/vb abandoning/vbg public/nn education/nn ./.\n",
            "\n",
            "\n",
            "\tThere/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
            "The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "3f73483a3d332c4be88f54115d48beaa6351e090",
        "_cell_guid": "9adcf194-d863-4043-85d0-00743c2786c9",
        "collapsed": true,
        "id": "ay6KvhcdNoFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9afa4afa-b8e4-4a99-e136-93f137b8a493"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(brown.raw('cb01').strip()[:1000]) # First 1000 characters."
      ]
    },
    {
      "metadata": {
        "_uuid": "f293a3c986c26e82394bd58a68bf2a0843b40f80",
        "_cell_guid": "9c58248d-6e32-496f-843b-1a07e3d4afb8",
        "id": "lTehbaH-NoFj"
      },
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "You will see that **each word comes with a slash and a label** and unlike normal text, we see that **punctuations are separated from the word that comes before it**, e.g.\n",
        "\n",
        "> The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
        "\n",
        "<br>\n",
        "And we also see that the **each sentence is separated by a newline**:\n",
        "\n",
        "> There/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
        ">\n",
        "> The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
        "\n",
        "<br>\n",
        "That brings us to the next point on **sentence tokenization** and **word tokenization**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verás que **cada palabra viene con una barra y una etiqueta** y a diferencia del texto normal, vemos que **las puntuaciones están separadas de la palabra que viene antes**, p. ej.\n",
        "La/a Asamblea/a General/a,/, que/a se levanta/a vbz hoy/a,/,/a ha/a actuado/a vbn en/a atmósfera/a de/a crisis/a y/a lucha/a desde/a el/a día/a en que/a convocó/a vbd ./.\n",
        "\n",
        "Y también vemos que **cada oración está separada por una nueva línea**:\n",
        "Allí/ex siguió/vbd la/at histórica/jj apropiaciones/nns y/cc presupuesto/nn lucha/nn ,/, en/en la cual/wdt la/at Asamblea/nn General/nn-tl decidió/vbd abordar/vb ejecutivo/nn poderes/nns ./.\n",
        "\n",
        "La/at decisión/nn final/vbd fue/vbd al/at ejecutivo/nn pero/cc se/vbn ha abierto/un/at camino/nn para/en fortalecer/vbg procedimientos/nns de presupuestación/y/cc para/proporcionar/vb a los legisladores/nns la/nns información/nn que/ppss necesitan/vb ./.\n",
        "\n",
        "Eso nos lleva al siguiente punto sobre la **tokenización de oraciones** y la **tokenización de palabras**.\n"
      ],
      "metadata": {
        "id": "SyRnUKBx0ig5"
      }
    },
    {
      "metadata": {
        "_uuid": "2275a8deeace1a0080b4194d70a0e6f751026e2d",
        "_cell_guid": "50b08633-6665-44e1-a989-e8b5fd3f11df",
        "id": "A2z1KWMcNoFk"
      },
      "cell_type": "markdown",
      "source": [
        "Tokenization\n",
        "====\n",
        "\n",
        "**Sentence tokenization** is the process of  *splitting up strings into “sentences”*\n",
        "\n",
        "**Word tokenization** is the process of  *splitting up “sentences” into “words”*\n",
        "\n",
        "Lets play around with some interesting texts,  the `singles.txt` from `webtext` corpus. <br>\n",
        "They were some  **singles ads** from  http://search.classifieds.news.com.au/\n",
        "\n",
        "First, downoad the data with `nltk.download()`:\n",
        "\n",
        "```python\n",
        "nltk.download('webtext')\n",
        "```\n",
        "\n",
        "Then you can import with:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('webtext')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J89NzSR0mun",
        "outputId": "21885d84-e91c-48bd-b48f-55a06a8e3c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "0a0c9f31e40f7ef9f3172aad7b68dbe3d4cc1829",
        "_cell_guid": "3636768b-77ad-4f0a-922d-1cedd2a8b11a",
        "collapsed": true,
        "id": "zx0u4T6FNoFk"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk.corpus import webtext"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['firefox.txt',\n",
              " 'grail.txt',\n",
              " 'overheard.txt',\n",
              " 'pirates.txt',\n",
              " 'singles.txt',\n",
              " 'wine.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "metadata": {
        "_uuid": "f899b6ed306c96f7731f1acc51aaba4256ec59b7",
        "_cell_guid": "6f9fd322-e916-4b70-83c3-87078c66fb15",
        "collapsed": true,
        "id": "guGqw6WtNoFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1a1947-baaf-4d23-88a3-dd6095f2f5d4"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "webtext.fileids()"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\t25 SEXY MALE, seeks attrac older single lady, for discreet encounters.\n",
            "1:\t35YO Security Guard, seeking lady in uniform for fun times.\n",
            "2:\t40 yo SINGLE DAD, sincere friendly DTE seeks r/ship with fem age open S/E\n",
            "3:\t44yo tall seeks working single mum or lady below 45 fship rship. Nat Open\n",
            "4:\t6.2 35 yr old OUTGOING M seeks fem 28-35 for o/door sports - w/e away\n",
            "5:\tA professional business male, late 40s, 6 feet tall, slim build, well groomed, great personality, home owner, interests include the arts travel and all things good, Ringwood area, is seeking a genuine female of similar age or older, in same area or surrounds, for a meaningful long term rship. Looking forward to hearing from you all.\n",
            "6:\tABLE young man seeks, sexy older women. Phone for fun ready to play\n",
            "7:\tAFFECTIONATE LADY Sought by generous guy, 40s, mutual fulfillment\n",
            "8:\tARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n",
            "9:\tAMIABLE 43 y.o. gentleman with European background, 170 cm, medium build, employed, never married, no children. Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future. 29-39 y.o. Prefer non-smoker and living in Adelaide.\n",
            "10:\tARE YOU A COPPER REDHEAD? I am 36 y.o. and looking for companionship/friendship. I enjoy the AFL, animals and dining out.\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "89a84224c596cf23b3498a7d5f3dbbf74428ef9e",
        "_cell_guid": "7518ac61-756f-466b-8294-ca6056994415",
        "collapsed": true,
        "id": "7ryJiK0dNoFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ec19a4-bfb9-4f61-bdee-049846dd8c91"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Each line is one advertisement.\n",
        "for i, line in enumerate(webtext.raw('singles.txt').split('\\n')):\n",
        "    if i > 10: # Lets take a look at the first 10 ads.\n",
        "        break\n",
        "    print(str(i) + ':\\t' + line)"
      ]
    },
    {
      "metadata": {
        "_uuid": "c1086676ea2bd10932715c1dfc5ebca5f7765389",
        "_cell_guid": "683d51eb-41e1-41bf-9bd2-3314d435f330",
        "id": "SgSc8EzvNoFk"
      },
      "cell_type": "markdown",
      "source": [
        "# Lets zoom in on candidate no. 8"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "3f117b6fa1c9dc0150800cdbd632f4cee1c3fbde",
        "_cell_guid": "7b46c556-583a-4af2-95d4-93d4d4b55bd2",
        "collapsed": true,
        "id": "KK4o6RzPNoFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbea9c7-56a4-4085-f5c3-97e3184f302d"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "single_no8 = webtext.raw('singles.txt').split('\\n')[8]\n",
        "print(single_no8)"
      ]
    },
    {
      "metadata": {
        "_uuid": "5d8daeb0c88fa79b07730d3d95d4f66382ebb0ef",
        "_cell_guid": "f227e306-1fc8-4d0a-bebc-55014599b588",
        "id": "In2Ucu0wNoFk"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentence Tokenization\n",
        "<br>\n",
        "In NLTK, `sent_tokenize()` the default tokenizer function that you can use to split strings into \"*sentences*\".\n",
        "<br>\n",
        "\n",
        "It is using the [**Punkt algortihm** from Kiss and Strunk (2006)](http://www.mitpressjournals.org/doi/abs/10.1162/coli.2006.32.4.485)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En NLTK, `sent_tokenize()` es la función tokenizadora predeterminada que se puede utilizar para dividir cadenas en \"*oraciones*\". Utiliza el algoritmo Punkt de Kiss y Strunk (2006)."
      ],
      "metadata": {
        "id": "47Pb4M5A34Ux"
      }
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "a5430c319a9f9cc66f074405d24271fec49e77c5",
        "_cell_guid": "c49b219d-864f-4353-9535-648592f0d847",
        "collapsed": true,
        "id": "-_Zcs89INoFl"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBE6Hb5O5CBZ",
        "outputId": "66cb4da6-e89c-4962-afd3-6afe90df46fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
              " 'Maybe we could explore new beginnings together?',\n",
              " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
              " 'You WONT be disappointed.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {
        "_uuid": "1d1a02d8bb0892942a4a5059a46b7f8aa23b7e01",
        "_cell_guid": "0666bda0-ebec-4f2b-ab92-2158b70e38a2",
        "collapsed": true,
        "id": "scR2nMWgNoFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6725138-1ac7-4e45-b45e-600257147035"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sent_tokenize(single_no8)"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
            "['Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
            "['Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
            "['You', 'WONT', 'be', 'disappointed', '.']\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "f1198990be58fd7d81cb6516651f98a2716824c3",
        "_cell_guid": "17ab6433-7166-4e87-837a-74cde568f98f",
        "collapsed": true,
        "id": "e3VUJbWeNoFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d4bcb4-c30a-417e-e103-bc7f69116baa"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for sent in sent_tokenize(single_no8):\n",
        "    print(word_tokenize(sent))"
      ]
    },
    {
      "metadata": {
        "_uuid": "0ee17d9e697179e1db2bd8ef95cae55f3b5d64b3",
        "_cell_guid": "e11cdead-66ae-4e25-9e68-9130297e0758",
        "id": "lZgrghgXNoFl"
      },
      "cell_type": "markdown",
      "source": [
        "# Lowercasing\n",
        "\n",
        "The CAPS in the texts are RATHER irritating although we KNOW the guy is trying to EMPHASIZE on something ;P\n",
        "\n",
        "We can simply **lowercase them after we do `sent_tokenize()` and `word_tokenize()`**. <br>\n",
        "The tokenizers uses the capitalization as cues to know when to split so removing them before the calling the functions would be sub-optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **lowercasing**\n",
        "Los límites en los textos son bastante irritantes, aunque sabemos que el tipo está tratando de enfatizar en algo; P\n",
        "\n",
        "Simplemente podemos minúsculas después de hacer sent_tokenize () y word_tokenize ().\n",
        "Los tokenizers usan la capitalización como señales para saber cuándo dividirse, así que eliminarlos antes de llamar a las funciones sería subóptimo."
      ],
      "metadata": {
        "id": "TQj5moWk5MHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize (single_no8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5SZ9BIN51Ky",
        "outputId": "d70b112a-8976-496b-9344-ff8e58cd2b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARE',\n",
              " 'YOU',\n",
              " 'ALONE',\n",
              " 'or',\n",
              " 'lost',\n",
              " 'in',\n",
              " 'a',\n",
              " 'r/ship',\n",
              " 'too',\n",
              " ',',\n",
              " 'with',\n",
              " 'no',\n",
              " 'hope',\n",
              " 'in',\n",
              " 'sight',\n",
              " '?',\n",
              " 'Maybe',\n",
              " 'we',\n",
              " 'could',\n",
              " 'explore',\n",
              " 'new',\n",
              " 'beginnings',\n",
              " 'together',\n",
              " '?',\n",
              " 'Im',\n",
              " '45',\n",
              " 'Slim/Med',\n",
              " 'build',\n",
              " ',',\n",
              " 'GSOH',\n",
              " ',',\n",
              " 'high',\n",
              " 'needs',\n",
              " 'and',\n",
              " 'looking',\n",
              " 'for',\n",
              " 'someone',\n",
              " 'similar',\n",
              " '.',\n",
              " 'You',\n",
              " 'WONT',\n",
              " 'be',\n",
              " 'disappointed',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
              " 'Maybe we could explore new beginnings together?',\n",
              " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
              " 'You WONT be disappointed.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "metadata": {
        "_uuid": "197cae1396cc555d02eb64676471a564e1e7f35a",
        "_cell_guid": "665cff0c-7140-444a-8ea9-ce3e20299464",
        "collapsed": true,
        "id": "EYvfrY7FNoFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05b9c43-af02-444f-93e2-5bb997be7d51"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sent_tokenize(single_no8)"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
            "['maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
            "['im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
            "['you', 'wont', 'be', 'disappointed', '.']\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "9f54f59d10281ff66ec36648cff7eda1f13f5ba2",
        "_cell_guid": "42c2c2d9-cb6b-41d4-ac89-46440b13e94c",
        "collapsed": true,
        "id": "zRAb1qkWNoFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f60068-d08e-49c1-b211-5cd7acc0885a"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for sent in sent_tokenize(single_no8):\n",
        "    # It's a little in efficient to loop through each word,\n",
        "    # after but sometimes it helps to get better tokens.\n",
        "    print([word.lower() for word in word_tokenize(sent)])\n",
        "    # Alternatively:\n",
        "    #print(list(map(str.lower, word_tokenize(sent))))"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'You', 'WONT', 'be', 'disappointed', '.']\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "b90631ac206dc4a6495729b06bcd1fc6415134d7",
        "_cell_guid": "08dea492-753c-4dba-8ac1-90c8dce48aef",
        "collapsed": true,
        "id": "TnLzAFHXNoFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d185bc-e487-401b-be24-7defd2cf91e6"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(word_tokenize(single_no8))  # Treats the whole line as one document."
      ]
    },
    {
      "metadata": {
        "_uuid": "7e5a3cc059f151e0ce4f975e56bc98205c03e90f",
        "_cell_guid": "eebfbac8-8b29-46ad-b2e2-e5badc926e4b",
        "id": "48vNm0OVNoFl"
      },
      "cell_type": "markdown",
      "source": [
        "# Tangential Note\n",
        "\n",
        "Punkt is a statistical model so it applies the knowledge it has learnt from previous data. <br>\n",
        "Generally, it **works for most cases on well-formed texts** but if your data is  different e.g. user-generated noisy texts, you might have to retrain a new model.\n",
        "![](http://)\n",
        "E.g. if we look at candidate no. 9, we see that it's splitting on `y.o.` (its thinking that its the end of the sentnence) and not splitting on `&c.` (its thinking that its an abbreviation, e.g. `Mr.`, `Inc.`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punkt es un modelo estadístico, por lo que aplica el conocimiento que ha aprendido de datos anteriores.\n",
        "Generalmente, funciona en la mayoría de los casos en textos bien formados, pero si sus datos son diferentes, por ejemplo, textos ruidosos generados por el usuario, es posible que tenga que volver a entrenar un nuevo modelo. Por ejemplo, si observamos al candidato n.° 9, vemos que se divide en y.o. (piensa que es el final de la oración) y no se divide en &c. (piensa que es una abreviatura, por ejemplo, Sr., Inc.)."
      ],
      "metadata": {
        "id": "DWaqABnr5dKl"
      }
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AMIABLE 43 y.o.',\n",
              " 'gentleman with European background, 170 cm, medium build, employed, never married, no children.',\n",
              " 'Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future.',\n",
              " '29-39 y.o.',\n",
              " 'Prefer non-smoker and living in Adelaide.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "metadata": {
        "_uuid": "ef345b667df7113cba651ae8edbeb0e213ee6809",
        "_cell_guid": "e3843142-7ff7-4e6d-8430-191ee8b4ce47",
        "collapsed": true,
        "id": "FZnpKjM1NoFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc4a980-8cdc-4c70-ff4d-89eb0799c305"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "single_no9 = webtext.raw('singles.txt').split('\\n')[9]\n",
        "sent_tokenize(single_no9)"
      ]
    },
    {
      "metadata": {
        "_uuid": "df75cb932724a29599f66c6229ba3f324a690181",
        "_cell_guid": "3d1a2d29-ab44-44f7-803a-ac3561368f09",
        "id": "TEtI5_dONoFm"
      },
      "cell_type": "markdown",
      "source": [
        "Stopwords\n",
        "====\n",
        "\n",
        "**Stopwords** are non-content words that primarily has only grammatical function\n",
        "\n",
        "In NLTK, you can access them as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las palabras vacías son palabras sin contenido que tienen principalmente una función gramatical.\n",
        "\n",
        "En NLTK, puedes acceder a ellas de la siguiente manera:"
      ],
      "metadata": {
        "id": "E8YTmlKP6IMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7aQU-666CX5",
        "outputId": "75f0d287-4bc6-481c-af9c-0f4764a72155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "501da9982fc07b1deaae173ed816caa01ac2dd79",
        "_cell_guid": "450cc60b-4e90-491f-9dd3-92ae22fd979a",
        "collapsed": true,
        "id": "KCBezvInNoFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468d31b3-a068-411b-da02-766630fd3845"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_en = stopwords.words('english')\n",
        "print(stopwords_en)"
      ]
    },
    {
      "metadata": {
        "_uuid": "e613de7ef925754b74334b79788acf4648b9be0b",
        "_cell_guid": "e02fc6ff-713d-4090-b7e2-3132c9415549",
        "id": "-SHxlL1KNoFm"
      },
      "cell_type": "markdown",
      "source": [
        "# Often we want to remove stopwords when we want to keep the \"gist\" of the document/sentence.\n",
        "\n",
        "For instance, lets go back to the our `single_no8`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A menudo queremos eliminar palabras vacías cuando queremos conservar la \"esencia\" del documento/oración."
      ],
      "metadata": {
        "id": "NOaJUFLd6Uak"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'you', 'wont', 'be', 'disappointed', '.']\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "ceef2a1d450726b705b22698b5b35635c942be38",
        "_cell_guid": "17a72a56-a94e-4d0b-af61-370d5f7b3adb",
        "collapsed": true,
        "id": "hOkAn5BhNoFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c30fd8-893a-4e71-e866-ff81ca6342c2"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Treat the multiple sentences as one document (no need to sent_tokenize)\n",
        "# Tokenize and lowercase\n",
        "single_no8_tokenized_lowered = list(map(str.lower, word_tokenize(single_no8)))\n",
        "print(single_no8_tokenized_lowered)"
      ]
    },
    {
      "metadata": {
        "_uuid": "7232e7aacc09962b8b010f6463ff14566471f541",
        "_cell_guid": "ea203327-c900-4d54-a1cd-fb3d94a9f021",
        "id": "E7KxA7H-NoFm"
      },
      "cell_type": "markdown",
      "source": [
        "# Let's try to remove the stopwords using the English stopwords list in NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intentemos eliminar las palabras vacías utilizando la lista de palabras vacías en inglés en NLTK"
      ],
      "metadata": {
        "id": "y2z_9x106bg0"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alone', 'lost', 'r/ship', ',', 'hope', 'sight', '?', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'looking', 'someone', 'similar', '.', 'wont', 'disappointed', '.']\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "3b691c536a5487f33b92eceb94b34b8d12cac22f",
        "_cell_guid": "b97fa9b8-6366-40b6-9b0f-05a15fc3b93f",
        "collapsed": true,
        "id": "z-AXGKLgNoFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d71c026-4c12-4f4f-da88-d6f64b3ad50c"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n",
        "\n",
        "# List comprehension.\n",
        "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en])"
      ]
    },
    {
      "metadata": {
        "_uuid": "d99001efcddcd7a080d76f6064ce309e41542f86",
        "_cell_guid": "a6ebff46-17a3-47d0-bf1f-47461ee484a4",
        "collapsed": true,
        "id": "1PY5ivdDNoFm"
      },
      "cell_type": "markdown",
      "source": [
        "# Often, we want to remove the punctuations from the documents too.\n",
        "\n",
        "Since Python comes with \"batteries included\", we have string.punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A menudo, también queremos eliminar los signos de puntuación de los documentos.\n",
        "Dado que Python viene con \"baterías incluidas\", tenemos string.punctuation"
      ],
      "metadata": {
        "id": "oTUl93gs6ijt"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "991965258d7aeffa5f55420fdf4e6f4a83dbfd9e",
        "_cell_guid": "fd82dfe3-2221-4c0a-9d0d-1b14a5349b91",
        "collapsed": true,
        "id": "qrRmnDU0NoFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33bd627-d1a6-4bb0-fd85-d121d6855b8d"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from string import punctuation\n",
        "# It's a string so we have to them into a set type\n",
        "print('From string.punctuation:', type(punctuation), punctuation)"
      ]
    },
    {
      "metadata": {
        "_uuid": "78edf4a03bd59a7753ba5a0d9a238df439711516",
        "_cell_guid": "c83a6e53-e7f4-43c9-a0b7-522ca1e75e55",
        "id": "F2WItj2VNoFn"
      },
      "cell_type": "markdown",
      "source": [
        "# Combining the punctuation with the stopwords from NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combinando la puntuación con las palabras de parada de NLTK."
      ],
      "metadata": {
        "id": "Hbvx4W-662xL"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'an', 'what', '?', 'which', 'his', '=', 'is', 'you', 'up', 'further', 'very', 'haven', \"that'll\", 'from', '+', 'both', 'my', \"weren't\", 'too', \"shouldn't\", \"should've\", 'didn', 'am', 'once', 'hers', 'does', 'on', '{', '$', \"'\", \"don't\", 'has', \"haven't\", '}', 'isn', \"wouldn't\", 'm', '^', 'through', 'this', 'yourselves', 'shouldn', 'some', \"needn't\", \"she's\", 'between', '_', 'needn', 'ain', 'shan', \"shan't\", 'below', 'have', \"you've\", 'her', 'y', 'aren', 'after', 'just', \"isn't\", '[', 'again', 'why', 'same', 'where', 'of', 'out', 'theirs', \"didn't\", 'wasn', 'then', \"won't\", 'that', 'did', 'during', \"mightn't\", '`', 'they', '\\\\', 'above', 'who', 'its', 'itself', 'these', 'been', 'all', 's', \"hasn't\", '(', 'we', 'because', \"hadn't\", ';', 'at', 'be', 'but', \"you're\", \"wasn't\", '%', 'doing', \"doesn't\", '|', 'against', 'to', 't', '>', 'such', 'only', 'll', '~', 'each', 'ourselves', 'them', \"couldn't\", \"you'd\", 'wouldn', \"aren't\", 'any', 'no', 'over', 'until', 'and', 'into', 'there', '-', 'being', 'him', 'won', 'more', '\"', 'those', 'your', 'whom', \"you'll\", 've', ':', ')', '/', 'doesn', 'with', 'than', 'he', 'off', \"it's\", 'not', 'having', 'ma', 'can', 'will', 'their', 'were', 'our', 'so', 'do', 'are', 'here', 'how', 'it', '*', 'mustn', 'himself', 'while', 'for', 'in', 'when', 'had', 'don', ',', 'myself', 'down', 'she', 'nor', 'a', \"mustn't\", '&', 'few', 'herself', 'was', 'ours', 'if', 'or', 're', '@', 'should', 'd', 'hadn', '!', 'by', 'yours', 'themselves', '.', 'couldn', 'hasn', 'mightn', ']', 'as', '<', 'other', 'yourself', 'before', 'i', 'under', 'own', 'me', 'now', 'o', '#', 'most', 'the', 'weren', 'about'}\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "97c96c2b0df9d8cd1dc6c6cf6619ed7f09c3072c",
        "_cell_guid": "ffafed92-9c4c-4fc1-aa7b-89645edf4b8c",
        "collapsed": true,
        "id": "W9jJJl2jNoFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033dec9f-c993-49d2-a4b1-b540afb2a6c2"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
        "print(stopwords_en_withpunct)"
      ]
    },
    {
      "metadata": {
        "_uuid": "e6b6acafbca2e1c8f88fbda2313da9d9b4961cf4",
        "_cell_guid": "230db9d8-bb06-4e48-aad9-4c4609448c4a",
        "id": "a1gk_CcUNoFn"
      },
      "cell_type": "markdown",
      "source": [
        "# Removing stopwords with punctuations from Single no. 8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminar palabras de parada con puntuaciones del no. 8"
      ],
      "metadata": {
        "id": "fPfr7pwl68HY"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alone', 'lost', 'r/ship', 'hope', 'sight', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'needs', 'looking', 'someone', 'similar', 'wont', 'disappointed']\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "c37dbd0c8a7658dd20b2266276baf0e162204576",
        "_cell_guid": "8cc6b68c-29b1-48b2-baa1-bd3f9bc4e21f",
        "collapsed": true,
        "scrolled": true,
        "id": "wBHSi5RNNoFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dea327c-e9a2-4511-c034-f5ecb673b5e6"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en_withpunct])"
      ]
    },
    {
      "metadata": {
        "_uuid": "027cd197d81df7b34048c520a84270c74781a6f0",
        "_cell_guid": "31f5c735-c260-4a4d-870e-721ccca5cb8b",
        "id": "rYgfRqIYNoFo"
      },
      "cell_type": "markdown",
      "source": [
        "# Using a stronger/longer list of stopwords\n",
        "\n",
        "From the previous output, we have still dangly model verbs (i.e. 'could', 'wont', etc.).\n",
        "\n",
        "We can combine the stopwords we have in NLTK with other stopwords list we find online.\n",
        "\n",
        "Personally, I like to use `stopword-json` because it has stopwrds in 50 languages =) <br>\n",
        "https://github.com/6/stopwords-json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso de una lista más larga y sólida de palabras vacías\n",
        "De la salida anterior, todavía tenemos verbos modelo sueltos (es decir, 'podría', 'no quiere', etc.).\n",
        "\n",
        "Podemos combinar las palabras vacías que tenemos en NLTK con otras listas de palabras vacías que encontramos en línea.\n",
        "\n",
        "Personalmente, me gusta usar stopword-json porque tiene palabras vacías en 50 idiomas =)\n"
      ],
      "metadata": {
        "id": "fnKRbuTr7DLi"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With combined stopwords:\n",
            "['lost', 'r/ship', 'hope', 'sight', 'explore', 'beginnings', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappointed']\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "376d2a04068b557bc5080353a998aa52c199dccb",
        "_cell_guid": "adf1ed39-3116-46f0-92c0-6dfddcd904a0",
        "collapsed": true,
        "id": "_9DyEsu3NoFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c15810b-173a-44d8-cfbf-e003010cd1b7"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Stopwords from stopwords-json\n",
        "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
        "stopwords_json_en = set(stopwords_json['en'])\n",
        "stopwords_nltk_en = set(stopwords.words('english'))\n",
        "stopwords_punct = set(punctuation)\n",
        "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
        "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
        "\n",
        "# Remove the stopwords from `single_no8`.\n",
        "print('With combined stopwords:')\n",
        "print([word for word in single_no8_tokenized_lowered if word not in stoplist_combined])"
      ]
    },
    {
      "metadata": {
        "_uuid": "84c1307d0bbc465925ef9932eddcbcdfd95a17ca",
        "_cell_guid": "273f7eb4-77ff-4cb9-abf0-4254122fa2b7",
        "id": "peViQadhNoFo"
      },
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lemmatization\n",
        "\n",
        "Often we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n",
        "\n",
        "The stemming and lemmatization process are hand-written regex rules written find the root word.\n",
        "\n",
        " - **Stemming**: Trying to shorten a word with simple regex rules\n",
        "\n",
        " - **Lemmatization**: Trying to find the root word with linguistics rules (with the use of regexes)\n",
        "\n",
        "(See also: [Stemmers vs Lemmatizers](https://stackoverflow.com/q/17317418/610569) question on StackOverflow)\n",
        "\n",
        "There are various stemmers and one lemmatizer in NLTK, the most common being:\n",
        "\n",
        " - **Porter Stemmer** from [Porter (1980)](https://tartarus.org/martin/PorterStemmer/index.html)\n",
        " - **Wordnet Lemmatizer** (port of the Morphy: https://wordnet.princeton.edu/man/morphy.7WN.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A menudo queremos mapear las diferentes formas de la misma palabra a la misma raíz, por ejemplo, \"camina\", \"caminando\", \"caminado\" deberían ser todas iguales a \"caminar\".\n",
        "\n",
        "El proceso de lematización y derivación son reglas de expresiones regulares escritas a mano para encontrar la raíz de la palabra.\n",
        "\n",
        "Derivación: intentar acortar una palabra con reglas de expresiones regulares simples\n",
        "\n",
        "Lematización: intentar encontrar la raíz de la palabra con reglas lingüísticas (con el uso de expresiones regulares)\n",
        "\n",
        "(Ver también: Pregunta sobre lematizadores vs. lematizadores en StackOverflow)\n",
        "\n",
        "Hay varios lematizadores y un lematizador en NLTK, siendo los más comunes:\n",
        "\n",
        "Lematizador de Porter de Porter (1980)\n",
        "Lematizador de Wordnet (versión de Morphy: https://wordnet.princeton.edu/man/morphy.7WN.html)\n"
      ],
      "metadata": {
        "id": "cw6agRnM7m9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wordnet.princeton.edu/\n"
      ],
      "metadata": {
        "id": "x8GHLYVH7uD3"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walk\n",
            "walk\n",
            "walk\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "3dd75f548e7363aa69b9da126a886f7299e052be",
        "_cell_guid": "f41dab63-6133-4a51-9eb6-f87414b30c30",
        "collapsed": true,
        "id": "VFchdvPcNoFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b8c359-8b1e-4ed6-f602-c4b53fba2e33"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "for word in ['walking', 'walks', 'walked']:\n",
        "    print(porter.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAMSxmMB76CJ",
        "outputId": "32dcdbb5-7ca4-4aa8-cd48-97ca36f84735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walking\n",
            "walk\n",
            "walked\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "31f49462e63529ac6d25b16389ef8494343b4de4",
        "_cell_guid": "edf939e2-bc38-4c50-92db-fd8d958ad17d",
        "collapsed": true,
        "id": "eK1n2T4mNoFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09bb921-b9d8-4afc-f95b-b2d0edabfd76"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "for word in ['walking', 'walks', 'walked']:\n",
        "    print(wnl.lemmatize(word))"
      ]
    },
    {
      "metadata": {
        "_uuid": "d2fa9f729a413a1417461683aa667f55a112e4e2",
        "_cell_guid": "fe9ffb81-f53a-4c58-97b6-86e0d79c51ca",
        "id": "bagRd1-ONoFo"
      },
      "cell_type": "markdown",
      "source": [
        "# Gotcha! The lemmatizer is actually pretty complicated, it needs Parts of Speech (POS) tags.\n",
        "\n",
        "\n",
        "We won't cover what's POS today so I'll just show you how to \"whip\" the lemmatizer to do what you need.\n",
        "\n",
        "By default, the WordNetLemmatizer.lemmatize() function will assume that the word is a Noun if there's no explict POS tag in the input.\n",
        "\n",
        "First you need the pos_tag function to tag a sentence and using the tag convert it into WordNet tagsets and then put it through to the WordNetLemmatizer.\n",
        "\n",
        "**Note:** Lemmatization won't really work on single words alone without context or knowledge of its POS tag (i.e. we need to know whether the word is a noun, verb, adjective, adverb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hoy no cubriremos qué es POS, así que solo te mostraré cómo \"manipular\" el lematizador para que haga lo que necesitas.\n",
        "\n",
        "De manera predeterminada, la función WordNetLemmatizer.lemmatize() asumirá que la palabra es un sustantivo si no hay una etiqueta POS explícita en la entrada.\n",
        "\n",
        "Primero necesitas la función pos_tag para etiquetar una oración y usar la etiqueta para convertirla en conjuntos de etiquetas de WordNet y luego pasarla al WordNetLemmatizer.\n",
        "\n",
        "Nota: La lematización no funcionará realmente con palabras individuales sin contexto o conocimiento de su etiqueta POS (es decir, necesitamos saber si la palabra es un sustantivo, verbo, adjetivo o adverbio).\n"
      ],
      "metadata": {
        "id": "7OBrRgzf8a_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_EijCxL8fgt",
        "outputId": "f781bdeb-61b0-4e67-f7fa-945cba12096c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('He', 'PRP'), ('is', 'VBZ'), ('walking', 'VBG'), ('to', 'TO'), ('school', 'NN')]\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "0813197200488bb07b8e23f5281ae324b323e981",
        "_cell_guid": "0fa94379-f1b2-4228-b30c-001d2895b993",
        "collapsed": true,
        "id": "_uhi2Sp5NoFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e37e5e-5e64-48c5-cc58-3424d6b75103"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n' # if mapping isn't found, fall back to Noun.\n",
        "\n",
        "# `pos_tag` takes the tokenized sentence as input, i.e. list of string,\n",
        "# and returns a tuple of (word, tg), i.e. list of tuples of strings\n",
        "# so we need to get the tag from the 2nd element.\n",
        "\n",
        "walking_tagged = pos_tag(word_tokenize('He is walking to school'))\n",
        "print(walking_tagged)"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'be', 'walk', 'to', 'school']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "metadata": {
        "_uuid": "e59549b59e571e2368d7c08c80250a854259d1d1",
        "_cell_guid": "6a826e5e-1c16-46ad-ac52-916a2f4e5c41",
        "collapsed": true,
        "id": "kenew_wPNoFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b911b8-a345-4386-8b2b-3959dbcf5fd8"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged]"
      ]
    },
    {
      "metadata": {
        "_uuid": "034d83608a867c28c06b86c9dc129b2f88faf326",
        "_cell_guid": "46b8011c-beba-4e69-8305-f5b98659e679",
        "id": "fpD93WpwNoFp"
      },
      "cell_type": "markdown",
      "source": [
        "# Now, lets create a new lemmatization function for sentences given what we learnt above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, creemos una nueva función de lematización para oraciones dado lo que aprendimos anteriormente."
      ],
      "metadata": {
        "id": "rfJVIT5j8w9_"
      }
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'be', 'walk', 'to', 'school']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "metadata": {
        "_uuid": "4f7803ff5cbc3f6c0dc97ce9c353a0bec39a5696",
        "_cell_guid": "6fe6901c-88ca-438f-9a9a-b1f28128ffc4",
        "collapsed": true,
        "id": "I9JIsS1INoFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53dfb1a6-39f3-44cf-c0da-da7d6e3cc549"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n'\n",
        "\n",
        "def lemmatize_sent(text):\n",
        "    # Text input is string, returns lowercased strings.\n",
        "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
        "            for word, tag in pos_tag(word_tokenize(text))]\n",
        "\n",
        "lemmatize_sent('He is walking to school')"
      ]
    },
    {
      "metadata": {
        "_uuid": "6ad66c3cabdf30287f2f6b0ea5ccbfdcd37cf927",
        "_cell_guid": "346bab96-cf6c-4a91-b27d-bfced577ea15",
        "id": "LDymb7INNoFp"
      },
      "cell_type": "markdown",
      "source": [
        "# Lets try the `lemmatize_sent()` and remove stopwords from Single no. 8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, creemos una nueva función de lematización para oraciones dado lo que aprendimos anteriormente."
      ],
      "metadata": {
        "id": "KWUXAw-u82r4"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Single no. 8:\n",
            "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed. \n",
            "\n",
            "Lemmatized and removed stopwords:\n",
            "['lose', 'r/ship', 'hope', 'sight', 'explore', 'beginning', 'im', 'slim/med', 'build', 'gsoh', 'high', 'similar', 'wont', 'disappoint']\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "e591e6f136501cee25961fb1df0e68f1eb676b45",
        "_cell_guid": "926f7ec4-12c7-4d8d-8f2b-ef66b0b3a5a4",
        "collapsed": true,
        "id": "PASP--X9NoFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98786e24-1ed1-4868-ad02-4ed3315b5893"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print('Original Single no. 8:')\n",
        "print(single_no8, '\\n')\n",
        "print('Lemmatized and removed stopwords:')\n",
        "print([word for word in lemmatize_sent(single_no8)\n",
        "       if word not in stoplist_combined\n",
        "       and not word.isdigit() ])"
      ]
    },
    {
      "metadata": {
        "_uuid": "56629a24758169967e39e8a93576195fb7c27ee8",
        "_cell_guid": "9c245e23-b526-4d86-9082-912c1aa87255",
        "id": "_nKQPnf9NoFq"
      },
      "cell_type": "markdown",
      "source": [
        "# Combining what we know about removing stopwords and lemmatization"
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "3145c9a5568eb2ed075eddf5a617d20b2286f012",
        "_cell_guid": "358ca69f-e7b2-4b93-ad09-5cf1b0a7143a",
        "collapsed": true,
        "id": "nMMHlOByNoFq"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def preprocess_text(text):\n",
        "    # Input: str, i.e. document/sentence\n",
        "    # Output: list(str) , i.e. list of lemmas\n",
        "    return [word for word in lemmatize_sent(text)\n",
        "            if word not in stoplist_combined\n",
        "            and not word.isdigit()]"
      ]
    },
    {
      "metadata": {
        "_uuid": "099da61cd361f9128632758422960862742b917f",
        "_cell_guid": "51ffeed8-ea00-479e-bc92-422f4e0bba0e",
        "id": "b2TkDyXZNoFq"
      },
      "cell_type": "markdown",
      "source": [
        "# Tangential Note on Lemmatization\n",
        "\n",
        "In English, a root word / lemma can manifest in different forms.\n",
        "\n",
        "| <img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"> | <img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"><img src=\"https://media1.giphy.com/media/xHHsXH7WJsWK4/giphy.gif\" align=\"left\" height=\"200\" width=\"200\"> |\n",
        "|:-------------:|:-------------:|\n",
        "| 1 cat  | 2 cats  |\n",
        "| 1 cat  | 2 cats  |\n",
        "\n",
        "For instance, we use “cat” to refer to a single “cat” and we attach an “-s”  suffix to refer to more than one cat, e.g. “two cats”.\n",
        "\n",
        "| <img src=\"https://68.media.tumblr.com/b0755247c8f32f79413d34b0410ccff1/tumblr_o3q8wlGi9v1u9ia8fo1_500.gif\" align=\"left\" height=\"200\" width=\"400\"> |\n",
        "|:-------------:|\n",
        "| cats walk / cats (are) walking |\n",
        "\n",
        "<!-- | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/ModelsCatwalk.jpg/440px-ModelsCatwalk.jpg\" align=\"left\" height=\"200\" width=\"400\"> |\n",
        "|:-------------:|\n",
        "| cat walk(s) / catwalk(s) |  -->\n",
        "\n",
        "\n",
        "Another example, the word “walk” has different forms, e.g. “walking” and “walked” indicate the time and/or progress of the walking motion. <!-- ~~Additionally, “walk” can also refer to the act of walking which is different from the walking motion, e.g. \"John went for a walk\" (act of walking) vs \"John wanted to walk to the park\" (the walking action/motion).~~ --> We refer to these root words as ***word types*** (e.g. “cat” and “walk”) and their different forms as ***word tokens*** (e.g. “cats”, “walk”, “walking”, “walked”, “walks”).\n",
        "\n",
        "Linguists further distinguish words between their lemmas or word families. A lemma refers to the canonical root word used as a dictionary entry. A word family refers to a group of lemmas which are derived from a single root word. Even though \"walkable\" would be a separate entry in a dictionary from \"walk\", \"walkable\" can be grouped under the word family of \"walk\" together with \"walking, walked, walks\".\n",
        "\n",
        "The distinction is subtle yet linguists go into great length to argue for what counts as a type, token, lemmas or word family."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otro ejemplo: la palabra “caminar” tiene diferentes formas, por ejemplo, “caminar” y “caminar” indican el tiempo y/o el progreso del movimiento de caminar. Nos referimos a estas raíces como tipos de palabras (por ejemplo, “gato” y “caminar”) y a sus diferentes formas como símbolos de palabras (por ejemplo, “gatos”, “caminar”, “caminar”, “caminar”, “caminar”).\n",
        "\n",
        "Los lingüistas distinguen además las palabras entre sus lemas o familias de palabras. Un lema se refiere a la raíz canónica de la palabra que se utiliza como entrada del diccionario. Una familia de palabras se refiere a un grupo de lemas que se derivan de una sola raíz de palabra. Aunque “caminable” sería una entrada separada de “caminar” en un diccionario, “caminable” se puede agrupar bajo la familia de palabras de “caminar” junto con “caminar, caminó, camina”.\n",
        "\n",
        "La distinción es sutil, pero los lingüistas se esfuerzan mucho para argumentar qué se considera un tipo, símbolo, lema o familia de palabras.\n"
      ],
      "metadata": {
        "id": "dCI0dusH9W9U"
      }
    },
    {
      "metadata": {
        "_uuid": "3f40b6cb3f60bc2bbfeebd27de0802d0bf7849cf",
        "_cell_guid": "9b00681d-5757-4547-828f-5767bfe701bd",
        "id": "NBqxk8wbNoFq"
      },
      "cell_type": "markdown",
      "source": [
        "# From Strings to Vectors\n",
        "\n",
        "**Vector** is an array of numbers\n",
        "\n",
        "**Vector Space Model** is conceptualizing language as a whole lot of numbers\n",
        "\n",
        "**Bag-of-Words (BoW)**: Counting each document/sentence as a vector of numbers, with each number representing the count of a word in the corpus\n",
        "\n",
        "To count, we can use the Python `collections.Counter`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **El vector es una matriz de números**\n",
        "\n",
        "El modelo de espacio vectorial conceptualiza el lenguaje como un conjunto de números\n",
        "\n",
        "Bolsa de palabras (BoW): contar cada documento/oración como un vector de números, donde cada número representa el recuento de una palabra en el corpus\n",
        "\n",
        "Para contar, podemos usar las colecciones de Python.Counter\n"
      ],
      "metadata": {
        "id": "UU222If79lsO"
      }
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "e5527ef7e753e6b3981c9909b50fcf39a428a85c",
        "_cell_guid": "37963ad4-b911-4bfc-b723-f6b7927a1eeb",
        "collapsed": true,
        "id": "jXtwKhjrNoFq"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from collections import Counter\n",
        "\n",
        "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "# Lemmatize and remove stopwords\n",
        "processed_sent1 = preprocess_text(sent1)\n",
        "processed_sent2 = preprocess_text(sent2)"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sentence:\n",
            "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
            "\n",
            "Word counts:\n",
            "Counter({'brown': 2, 'quick': 1, 'fox': 1, 'jump': 1, 'lazy': 1, 'dog': 1})\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "cfb48c07796e6b3fbe20378dcef1f11e4880b1f9",
        "_cell_guid": "882d59c1-e3bf-4e72-84e2-858490ec66b7",
        "collapsed": true,
        "id": "iKJJxrEfNoFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce67e20e-5b12-4e22-c146-32ff5cb4e095"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print('Processed sentence:')\n",
        "print(processed_sent1)\n",
        "print()\n",
        "print('Word counts:')\n",
        "print(Counter(processed_sent1))"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sentence:\n",
            "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
            "\n",
            "Word counts:\n",
            "Counter({'mr': 1, 'brown': 1, 'jump': 1, 'lazy': 1, 'fox': 1})\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "4e9bd5f5852015cf8f004d1f7a01274b76cb033d",
        "_cell_guid": "75e94392-cf5c-40f5-bcd7-349e4861fab7",
        "collapsed": true,
        "id": "kNCsoDCfNoFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfca38ee-b526-402d-ba92-dd1d89d7e4cd"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print('Processed sentence:')\n",
        "print(processed_sent2)\n",
        "print()\n",
        "print('Word counts:')\n",
        "print(Counter(processed_sent2))"
      ]
    },
    {
      "metadata": {
        "_uuid": "25205be78a242dcb9b8bd653cd98244ce73ba69b",
        "_cell_guid": "e51b4c01-2712-4ec9-852b-133716681b53",
        "id": "MJzuZZp3NoFr"
      },
      "cell_type": "markdown",
      "source": [
        "# Vectorization\n",
        "\n",
        "Let's put the words and counts into a nice table:\n",
        "\n",
        "| | brown | quick | fox | jump | lazy | dog | mr |\n",
        "|:---- |:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
        "| Sent1 | 2 | 1 | 1 | 1 | 1 | 1 | 0 |  \n",
        "| Sent2 | 1 | 0 | 1 | 1 | 1 | 0 | 1 |\n",
        "\n",
        "\n",
        "If we fix the positions of the vocabulary i.e.\n",
        "\n",
        "```\n",
        "[brown, quick, fox, jump, lazy, dog, mr]\n",
        "```\n",
        "\n",
        "and we do the counts for each word in each sentence, we get the sentence vectors (i.e. list of numbers to represent each sentence):\n",
        "\n",
        "```\n",
        "sent1 = [2,1,1,1,1,1,0]\n",
        "sent2 = [1,0,1,1,1,0,1]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pongamos las palabras y los recuentos en una bonita tabla:"
      ],
      "metadata": {
        "id": "cuSDZTJo9z1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si fijamos las posiciones del vocabulario, es decir,"
      ],
      "metadata": {
        "id": "C74CBq7J99OR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "y hacemos los recuentos para cada palabra en cada oración, obtenemos los vectores de oración (es decir, la lista de números para representar cada oración):"
      ],
      "metadata": {
        "id": "R1v61fPG-FjI"
      }
    },
    {
      "metadata": {
        "_uuid": "5c268c3e117c595b2835cdfd7c5abe604f35e0fa",
        "_cell_guid": "64860ccd-4cf4-46dd-89d2-3929d9b7d539",
        "id": "hqNHODEWNoFr"
      },
      "cell_type": "markdown",
      "source": [
        "# Vectorization with sklearn\n",
        "\n",
        "In `scikit-learn`, there're pre-built functions to do the preprocessing and vectorization that we've been doing using the `CountVectorizer` object.\n",
        "\n",
        "It will be the object that contains the vocabulary (i.e. the first row of our table above) and has the function to convert any sentence into the counts vectors we see as above.\n",
        "\n",
        "The input that `CountVectorizer` is a textfile, so we've to do some hacking to put let it accept the string outputs.\n",
        "\n",
        "We can \"fake it to make it\" using `io.StringIO` where we can convert any string to work like a file, e.g."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En Scikit-Learn, hay funciones preconstruidas para hacer el preprocesamiento y la vectorización que hemos estado haciendo utilizando el objeto CountVectorizer.\n",
        "\n",
        "Será el objeto que contiene el vocabulario (es decir, la primera fila de nuestra tabla anterior) y tiene la función de convertir cualquier oración en los vectores de recuentos que vemos como anteriormente.\n",
        "\n",
        "La entrada que CountVectorizer es un archivo de texto, por lo que tenemos que hacer un poco de piratería para ponerlo de que acepte las salidas de cadena.\n",
        "\n",
        "Podemos \"fingirlo para hacerlo\" usando io.stringio donde podemos convertir cualquier cadena para funcionar como un archivo, p.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vzy6IcwV-R17"
      }
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "3834e62e0c3879b37f94e1924e80534a5899fbfd",
        "_cell_guid": "5f92d415-5016-4405-b797-d98df2393a89",
        "collapsed": true,
        "id": "tKgFvj0JNoFr"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
        "    # Create the vectorizer\n",
        "    count_vect = CountVectorizer()\n",
        "    count_vect.fit_transform(fin)"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 8,\n",
              " 'quick': 7,\n",
              " 'brown': 0,\n",
              " 'fox': 2,\n",
              " 'jumps': 3,\n",
              " 'over': 6,\n",
              " 'lazy': 4,\n",
              " 'dog': 1,\n",
              " 'mr': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "metadata": {
        "_uuid": "36407729e4ed2636089293545a5f2d030a4d60e8",
        "_cell_guid": "a8a7db77-9578-4f58-8c09-8cde03f6a865",
        "collapsed": true,
        "id": "KGHR5Fr9NoFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e632991f-daad-4035-ede5-1b01dc475807"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# We can check the vocabulary in our vectorizer\n",
        "# It's a dictionary where the words are the keys and\n",
        "# The values are the IDs given to each word.\n",
        "count_vect.vocabulary_"
      ]
    },
    {
      "metadata": {
        "_uuid": "e2d864d079623c6d92e2694dad75bd8edf810d48",
        "_cell_guid": "13d2b03b-6ab9-4da4-b20e-38c3cce08243",
        "id": "jmtPqDWWNoFs"
      },
      "cell_type": "markdown",
      "source": [
        "**Note:** We haven't counted anything yet just initializing our vectorizer object with the vocabulary."
      ]
    },
    {
      "metadata": {
        "_uuid": "07b29c69132ed41c5da7f869f098694374b42cda",
        "_cell_guid": "2249db3e-0b22-4f63-be8e-b19c951a4ea2",
        "id": "REF7TObiNoFs"
      },
      "cell_type": "markdown",
      "source": [
        "# ちょっと待ってください ... (Wait a minute)\n",
        "\n",
        "I didn't tell the vectorizer to remove punctuation and tokenize and lowercase, how did they do it?\n",
        "\n",
        "Also, `the` is in the vocabulary, it's a stopword, we want it gone... <br>\n",
        "And `jumps` isn't stemmed or lemmatized!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No le dije al vectorizador que elimine la puntuación y la tokenización y en minúsculas, ¿cómo lo hicieron?\n",
        "\n",
        "Además, está en el vocabulario, es una palabra de parada, queremos que se haya ido ...\n",
        "¡Y los saltos no tienen tallo o lematizado!"
      ],
      "metadata": {
        "id": "9iTp6C9tBZsl"
      }
    },
    {
      "metadata": {
        "_uuid": "e6393e0f95e0b0dbd92f64f22883331f9eaac919",
        "_cell_guid": "035f1601-9e72-4d85-929e-44d8447d7253",
        "id": "PW2EHlEpNoFs"
      },
      "cell_type": "markdown",
      "source": [
        "If we look at the documentation of the [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in `sklearn`, we see:\n",
        "\n",
        "\n",
        "```python\n",
        "CountVectorizer(\n",
        "    input=’content’, encoding=’utf-8’,\n",
        "    decode_error=’strict’, strip_accents=None,\n",
        "    lowercase=True, preprocessor=None,\n",
        "    tokenizer=None, stop_words=None,\n",
        "    token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1),\n",
        "    analyzer=’word’, max_df=1.0, min_df=1,\n",
        "    max_features=None, vocabulary=None,\n",
        "    binary=False, dtype=<class ‘numpy.int64’>)[source]\n",
        "```\n",
        "\n",
        "And more specifically:\n",
        "\n",
        "> **analyzer** : string, {‘word’, ‘char’, ‘char_wb’} or callable\n",
        ">\n",
        "> Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
        "> If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n",
        "\n",
        "\n",
        "> **preprocessor** : callable or None (default)\n",
        ">\n",
        "> Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n",
        "\n",
        "> **tokenizer** : callable or None (default)\n",
        ">\n",
        "> Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word'.\n",
        "\n",
        "> **stop_words** : string {‘english’}, list, or None (default)\n",
        ">\n",
        "> If ‘english’, a built-in stop word list for English is used.\n",
        "> If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.\n",
        "If None, no stop words will be used.\n",
        "\n",
        "> **lowercase** : boolean, True by default\n",
        ">\n",
        "> Convert all characters to lowercase before tokenizing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "analyzer : string, {‘word’, ‘char’, ‘char_wb’} o callable\n",
        "\n",
        "Si la característica debe estar formada por n-gramas de palabras o caracteres. La opción ‘char_wb’ crea n-gramas de caracteres solo a partir del texto dentro de los límites de las palabras; los n-gramas en los bordes de las palabras se rellenan con espacios. Si se pasa un callable, se utiliza para extraer la secuencia de características de la entrada sin procesar.\n",
        "\n",
        "preprocessor : callable o None (predeterminado)\n",
        "\n",
        "Anula la etapa de preprocesamiento (transformación de cadenas) mientras se conservan los pasos de tokenización y generación de n-gramas.\n",
        "\n",
        "tokenizer : callable o None (predeterminado)\n",
        "\n",
        "Anula el paso de tokenización de cadenas mientras se conservan los pasos de preprocesamiento y generación de n-gramas. Solo se aplica si analyzer == 'word'.\n",
        "\n",
        "stop_words : cadena {‘english’}, lista o None (predeterminado)\n",
        "\n",
        "Si es ‘english’, se utiliza una lista de palabras vacías incorporada para inglés. Si es una lista, se supone que esa lista contiene palabras vacías, todas las cuales se eliminarán de los tokens resultantes. Solo se aplica si analyzer == 'word'. Si es None, no se utilizarán palabras vacías.\n",
        "\n",
        "lowercase : booleano, True de forma predeterminada\n",
        "\n",
        "Convierte todos los caracteres a minúsculas antes de convertirlos en tokens."
      ],
      "metadata": {
        "id": "VCYcTwr-Bo1n"
      }
    },
    {
      "metadata": {
        "_uuid": "11d7d8b6406c5d56259d72c48f97747e119885b7",
        "_cell_guid": "9eb50e4d-4d52-47a4-9607-fd33fe3775d1",
        "id": "2Rm6I_QLNoFs"
      },
      "cell_type": "markdown",
      "source": [
        "# Achso, we can override these arguments with the functions we have learnt before."
      ]
    },
    {
      "metadata": {
        "_uuid": "4875ebac4a09758236313d7ffa17fb015faac926",
        "_cell_guid": "e9c00de2-ab64-4a35-a790-485a64b43dd3",
        "id": "Kt1S5Zp-NoFs"
      },
      "cell_type": "markdown",
      "source": [
        "We can **override the tokenizer and stop_words**:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos anular el tokenizer y stop_words:"
      ],
      "metadata": {
        "id": "_4E5hqb3B6dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "# Assuming stoplist_combined is a set, convert it to a list\n",
        "stop_words_list = list(stoplist_combined)\n",
        "\n",
        "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
        "    # Override the analyzer totally with our preprocess text\n",
        "    count_vect = CountVectorizer(stop_words=stop_words_list, # Using the list here\n",
        "                                 tokenizer=word_tokenize)\n",
        "    count_vect.fit_transform(fin)\n",
        "count_vect.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gquRvMLMCLLp",
        "outputId": "5beec149-6214-4183-bf4b-6526505f08fa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '``', 'ai', 'ca', \"n't\", 'sha', 'wo'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'quick': 6, 'brown': 0, 'fox': 2, 'jumps': 3, 'lazy': 4, 'dog': 1, 'mr': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e23d630dd87abeae6ed1b37d64c2fe5223da7088",
        "_cell_guid": "5f29251e-1074-4c04-a50c-d56d4ca3a97e",
        "id": "Z9jXSwnpNoFt"
      },
      "cell_type": "markdown",
      "source": [
        "Or just **override the analyzer** totally with our preprocess text:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O simplemente anule totalmente el analizador con nuestro texto de preprocesamiento:"
      ],
      "metadata": {
        "id": "7ANhTsU1CQir"
      }
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'quick': 6, 'brown': 0, 'fox': 2, 'jump': 3, 'lazy': 4, 'dog': 1, 'mr': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "metadata": {
        "_uuid": "fd10c8df7c870641807211b97140900e570c46af",
        "_cell_guid": "40813b12-f701-4bca-a2e3-c16b928dd32e",
        "collapsed": true,
        "id": "C7e3MX3eNoFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7e8bc4-e933-4829-d889-359d4bb1f66b"
      },
      "cell_type": "code",
      "execution_count": 59,
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
        "    # Override the analyzer totally with our preprocess text\n",
        "    count_vect = CountVectorizer(analyzer=preprocess_text)\n",
        "    count_vect.fit_transform(fin)\n",
        "count_vect.vocabulary_"
      ]
    },
    {
      "metadata": {
        "_uuid": "4024585e83698a0c454b6ff332de4a32d8bfeb8b",
        "_cell_guid": "ab69e27d-1d13-47f0-a547-9e234292d166",
        "id": "cbieZT2MNoFt"
      },
      "cell_type": "markdown",
      "source": [
        "# To vectorize any new sentences, we use  `CountVectorizer.transform()`\n",
        "\n",
        "The function  will return a sparse matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para vectorizar cualquier oración nueva, utilizamos CountVectorizer.transform()\n",
        "La función devolverá una matriz dispersa."
      ],
      "metadata": {
        "id": "CB0pDbQfCY3h"
      }
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 11 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "metadata": {
        "_uuid": "e18dbaa0882825ffc4b1b746de13eb4d8381984d",
        "_cell_guid": "92d41c12-7f75-406e-80ec-66a2977a79a8",
        "collapsed": true,
        "id": "A_r4HbPWNoFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834410cb-8f6c-4eeb-d930-1228eafbbdcb"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "count_vect.transform([sent1, sent2])"
      ]
    },
    {
      "metadata": {
        "_uuid": "8815c6acca7b396407004eaf858a957576c06673",
        "_cell_guid": "81e138ab-0b0f-4c67-abe9-b34c15cb7e8d",
        "id": "pB0fDP5fNoFt"
      },
      "cell_type": "markdown",
      "source": [
        "# To view the matrix, you can output it to an array"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ver la matriz, puedes generarla en una matriz"
      ],
      "metadata": {
        "id": "KYge4WNECdzM"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
            "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
            "\n",
            "Vocab: ('brown', 'dog', 'fox', 'jump', 'lazy', 'mr', 'quick')\n",
            "\n",
            "Matrix/Vectors:\n",
            " [[2 1 1 1 1 0 1]\n",
            " [1 0 1 1 1 1 0]]\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "ec34de51827ed00a2cdb39c000a03bb85e582713",
        "_cell_guid": "4e4029f4-1440-4ebd-b239-c365e735e21d",
        "collapsed": true,
        "id": "0n7jrdTSNoFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92d34e0-e628-4bbd-b8cc-7518c723df41"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "# Print the words sorted by their index\n",
        "words_sorted_by_index, _ = zip(*sorted(count_vect.vocabulary_.items(), key=itemgetter(1)))\n",
        "\n",
        "print(preprocess_text(sent1))\n",
        "print(preprocess_text(sent2))\n",
        "print()\n",
        "print('Vocab:', words_sorted_by_index)\n",
        "print()\n",
        "print('Matrix/Vectors:\\n', count_vect.transform([sent1, sent2]).toarray())"
      ]
    },
    {
      "metadata": {
        "_uuid": "0c271bfc242864673a862c0fc4e587ad57654528",
        "_cell_guid": "c520f436-9d65-4950-a7aa-91eaf9382d12",
        "id": "ND9N5U_VNoFt"
      },
      "cell_type": "markdown",
      "source": [
        "Naive Bayes\n",
        "====\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Classification\n",
        "====\n",
        "\n",
        "Classification simply means putting our data points into bins/box. You can also think of it as assigning label to our data points, e.g. given box of fruits, sort them in apples, oranges and others.\n",
        "\n",
        "Okay, the explanation could be more complex than that but `import this` says:\n",
        "\n",
        "> **Simple is better than complex.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Clasificación**\n",
        "La clasificación simplemente significa colocar nuestros puntos de datos en contenedores o cajas. También puede pensar en ello como asignar una etiqueta a nuestros puntos de datos, p. ej., dada una caja de frutas, clasifíquelas en manzanas, naranjas y otras.\n",
        "\n",
        "Bien, la explicación podría ser más compleja que eso, pero esto dice:\n",
        "\n",
        "Lo simple es mejor que lo complejo.\n"
      ],
      "metadata": {
        "id": "0jrZdGkxCpEy"
      }
    },
    {
      "metadata": {
        "_uuid": "5d163c46b118b580f07eeebad0a16eaf5edbb20d",
        "_cell_guid": "1fd3db77-b764-4f26-914f-ec0ac4e4d0c9",
        "id": "B5toE3RaNoFt"
      },
      "cell_type": "markdown",
      "source": [
        "# Now that we learnt some basic NLP and vectorization, lets apply it to a fun task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que aprendimos algunos PNL y vectorización básicos, aplicándolo a una tarea divertida."
      ],
      "metadata": {
        "id": "Mumj0HWtC_5h"
      }
    },
    {
      "metadata": {
        "_uuid": "26ce98337b4e3c9f54278412e578117bfd92d625",
        "_cell_guid": "0a6d1a71-4171-4384-a0b8-5cae9efbea33",
        "id": "OryrsviVNoFu"
      },
      "cell_type": "markdown",
      "source": [
        "[Random Acts of Pizza](https://www.kaggle.com/c/random-acts-of-pizza)\n",
        "=====\n",
        "\n",
        "In machine learning, it is often said there are [no free lunches](). How wrong we were.\n",
        "\n",
        "This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data.\n",
        "\n",
        "![](https://kaggle2.blob.core.windows.net/competitions/kaggle/3949/media/pizzas.png)\n",
        "\n",
        "The task is to create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el mundo del aprendizaje automático, se suele decir que no hay almuerzos gratis. ¡Qué equivocados estábamos!\n",
        "\n",
        "Esta competición contiene un conjunto de datos con 5671 pedidos de pizza en formato de texto de la comunidad de Reddit Random Acts of Pizza junto con su resultado (exitoso/no exitoso) y metadatos.\n",
        "\n",
        "La tarea consiste en crear un algoritmo capaz de predecir qué pedidos obtendrán un acto de bondad cursi (¡pero sincero!)."
      ],
      "metadata": {
        "id": "UiBIXwZsDLK9"
      }
    },
    {
      "metadata": {
        "_uuid": "3cae972238580fe9c4fb6a184b3661b232f77458",
        "_cell_guid": "4c85876b-1b19-4c50-8330-a2c9dbd91f32",
        "id": "1AuqyioQNoFu"
      },
      "cell_type": "markdown",
      "source": [
        "# Lets take a look at the training data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.github.com/content/Viny2030/datasets/blob/main/train.json\"\n",
        "!wget $url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E4vRV2xFLOm",
        "outputId": "f5fd7c18-5a05-461a-b569-792aa25d7c3b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-07 19:23:57--  https://raw.github.com/content/Viny2030/datasets/blob/main/train.json\n",
            "Resolving raw.github.com (raw.github.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.github.com (raw.github.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://raw.githubusercontent.com/content/Viny2030/datasets/blob/main/train.json [following]\n",
            "--2024-11-07 19:23:57--  https://raw.githubusercontent.com/content/Viny2030/datasets/blob/main/train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-11-07 19:23:57 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMkR7ZF5F9uy",
        "outputId": "f30399fe-0d92-48ed-a289-624d7188c1d8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "efab58829e4f5082cd9ac154953f73c1f5033763",
        "_cell_guid": "211f0cf5-8be5-482e-bc3c-d57fd089689c",
        "collapsed": true,
        "id": "MdKntjDlNoFu"
      },
      "cell_type": "code",
      "execution_count": 66,
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/UNED/notebooks/train.json') as fin:\n",
        "    trainjson = json.load(fin)"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'giver_username_if_known': 'N/A',\n",
              " 'number_of_downvotes_of_request_at_retrieval': 0,\n",
              " 'number_of_upvotes_of_request_at_retrieval': 1,\n",
              " 'post_was_edited': False,\n",
              " 'request_id': 't3_l25d7',\n",
              " 'request_number_of_comments_at_retrieval': 0,\n",
              " 'request_text': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
              " 'request_text_edit_aware': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
              " 'request_title': 'Request Colorado Springs Help Us Please',\n",
              " 'requester_account_age_in_days_at_request': 0.0,\n",
              " 'requester_account_age_in_days_at_retrieval': 792.4204050925925,\n",
              " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
              " 'requester_days_since_first_post_on_raop_at_retrieval': 792.4204050925925,\n",
              " 'requester_number_of_comments_at_request': 0,\n",
              " 'requester_number_of_comments_at_retrieval': 0,\n",
              " 'requester_number_of_comments_in_raop_at_request': 0,\n",
              " 'requester_number_of_comments_in_raop_at_retrieval': 0,\n",
              " 'requester_number_of_posts_at_request': 0,\n",
              " 'requester_number_of_posts_at_retrieval': 1,\n",
              " 'requester_number_of_posts_on_raop_at_request': 0,\n",
              " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
              " 'requester_number_of_subreddits_at_request': 0,\n",
              " 'requester_received_pizza': False,\n",
              " 'requester_subreddits_at_request': [],\n",
              " 'requester_upvotes_minus_downvotes_at_request': 0,\n",
              " 'requester_upvotes_minus_downvotes_at_retrieval': 1,\n",
              " 'requester_upvotes_plus_downvotes_at_request': 0,\n",
              " 'requester_upvotes_plus_downvotes_at_retrieval': 1,\n",
              " 'requester_user_flair': None,\n",
              " 'requester_username': 'nickylvst',\n",
              " 'unix_timestamp_of_request': 1317852607.0,\n",
              " 'unix_timestamp_of_request_utc': 1317849007.0}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "metadata": {
        "_uuid": "f363473d1aaaedd58eae40f7c970d1eedf6c7fa4",
        "_cell_guid": "0bb18893-12ed-4178-89cc-06c4f01d7107",
        "collapsed": true,
        "id": "hT1V_UX2NoFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98db977-e5a0-4f3a-bb65-1ce1905e4200"
      },
      "cell_type": "code",
      "execution_count": 67,
      "source": [
        "trainjson[0]"
      ]
    },
    {
      "metadata": {
        "_uuid": "d231a3cdad2807893f44de861892093d02cb07a7",
        "_cell_guid": "24d5a591-79a6-4740-8b58-c76fef459355",
        "id": "ipkEx2_QNoFw"
      },
      "cell_type": "markdown",
      "source": [
        "We're only interested in the text fields:\n",
        "\n",
        "**Input**:\n",
        " - `request_id`: unique identifier for the request\n",
        " - `request_title`: title of the reddit post for pizza request\n",
        " - `request_text_edit_aware`: expository to request for pizza\n",
        "\n",
        "**Output**:\n",
        " - `requester_recieved_pizza`: whether requester gets his/her pizza\n",
        "\n",
        "For our purpose, lets only use the `request_text` as the input to build our Naive Bayes classifier and the output is the `requester_recieved_pizza` field.\n",
        "\n",
        "**Note:** The `request_id` is only used for mapping purpose when we're submitting the results to the Kaggle task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solo nos interesan los campos de texto:\n",
        "\n",
        "Entrada:\n",
        "\n",
        "request_id: identificador único de la solicitud\n",
        "request_title: título de la publicación de Reddit para la solicitud de pizza\n",
        "request_text_edit_aware: explicación de la solicitud de pizza\n",
        "Salida:\n",
        "\n",
        "requester_recieved_pizza: si el solicitante recibe su pizza\n",
        "Para nuestro propósito, usemos solo el request_text como entrada para construir nuestro clasificador Naive Bayes y la salida es el campo requester_recieved_pizza.\n",
        "\n",
        "Nota: El request_id solo se usa para fines de mapeo cuando enviamos los resultados a la tarea de Kaggle."
      ],
      "metadata": {
        "id": "4_yx4YvMHjVW"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UID:\t t3_l25d7 \n",
            "\n",
            "Title:\t Request Colorado Springs Help Us Please \n",
            "\n",
            "Text:\t Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated \n",
            "\n",
            "Tag:\t False\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "9755249c3af76081f4203b75f2fbe05e860db4f6",
        "_cell_guid": "38df138d-858e-4983-be85-37f864b8b7aa",
        "collapsed": true,
        "id": "LohHn8CBNoFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fb07e9-40de-47d5-9446-14ab88f7305e"
      },
      "cell_type": "code",
      "execution_count": 69,
      "source": [
        "print('UID:\\t', trainjson[0]['request_id'], '\\n')\n",
        "print('Title:\\t', trainjson[0]['request_title'], '\\n')\n",
        "print('Text:\\t', trainjson[0]['request_text_edit_aware'], '\\n')\n",
        "print('Tag:\\t', trainjson[0]['requester_received_pizza'], end='\\n')"
      ]
    },
    {
      "metadata": {
        "_uuid": "bf75dd208d746cb254d08576477c25c2472d8cae",
        "_cell_guid": "2d3b4d73-784e-4aef-a1c2-d70a0592495e",
        "id": "2dl-w4WKNoFw"
      },
      "cell_type": "markdown",
      "source": [
        "# Here's a neat trick to convert json to pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Instead of pd.io.json.json_normalize, use pd.json_normalize directly:\n",
        "df = pd.json_normalize(trainjson)  # Pandas magic...\n",
        "\n",
        "df_train = df[['request_id', 'request_title',\n",
        "               'request_text_edit_aware',\n",
        "               'requester_received_pizza']]\n",
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oYt5orcWHv5Q",
        "outputId": "4efae6b3-4d74-4fb0-e2bf-ec39ea68c5b4"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  request_id                                      request_title  \\\n",
              "0   t3_l25d7            Request Colorado Springs Help Us Please   \n",
              "1   t3_rcb83  [Request] California, No cash and I could use ...   \n",
              "2   t3_lpu5j  [Request] Hungry couple in Dundee, Scotland wo...   \n",
              "3   t3_mxvj3  [Request] In Canada (Ontario), just got home f...   \n",
              "4  t3_1i6486  [Request] Old friend coming to visit. Would LO...   \n",
              "\n",
              "                             request_text_edit_aware  requester_received_pizza  \n",
              "0  Hi I am in need of food for my 4 children we a...                     False  \n",
              "1  I spent the last money I had on gas today. Im ...                     False  \n",
              "2  My girlfriend decided it would be a good idea ...                     False  \n",
              "3  It's cold, I'n hungry, and to be completely ho...                     False  \n",
              "4  hey guys:\\n I love this sub. I think it's grea...                     False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba571dc8-f857-42df-9199-04316ed12110\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>request_id</th>\n",
              "      <th>request_title</th>\n",
              "      <th>request_text_edit_aware</th>\n",
              "      <th>requester_received_pizza</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>t3_l25d7</td>\n",
              "      <td>Request Colorado Springs Help Us Please</td>\n",
              "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>t3_rcb83</td>\n",
              "      <td>[Request] California, No cash and I could use ...</td>\n",
              "      <td>I spent the last money I had on gas today. Im ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t3_lpu5j</td>\n",
              "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
              "      <td>My girlfriend decided it would be a good idea ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>t3_mxvj3</td>\n",
              "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
              "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>t3_1i6486</td>\n",
              "      <td>[Request] Old friend coming to visit. Would LO...</td>\n",
              "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba571dc8-f857-42df-9199-04316ed12110')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba571dc8-f857-42df-9199-04316ed12110 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba571dc8-f857-42df-9199-04316ed12110');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-62c22a56-97d4-44b9-82fa-ac8fa62c48c3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-62c22a56-97d4-44b9-82fa-ac8fa62c48c3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-62c22a56-97d4-44b9-82fa-ac8fa62c48c3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 4040,\n  \"fields\": [\n    {\n      \"column\": \"request_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4040,\n        \"samples\": [\n          \"t3_q8v2x\",\n          \"t3_ie8kp\",\n          \"t3_tjtxc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"request_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4025,\n        \"samples\": [\n          \"[REQUEST] - Location, Tennessee: Recently had my identity stolen, All accounts wiped out, really need some food.\",\n          \"[Request] Coquitlam, BC, Canada - Roommate hasn't paid rent in 2 months and I'm left alone with an empty fridge. Would greatly appreciate something to eat tonight.\",\n          \"[REQUEST] Just moved across country, no money for a few days\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"request_text_edit_aware\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3932,\n        \"samples\": [\n          \"Hello, I'm new to this subbreddit and I'm not exactly sure how this works. Essentially, I'm a broke college student with classes starting this Tuesday. I would like to get pizza for my family tonight just to be nice, but I can't afford it with the cost of books. 3/5 people in my family have celiac disease and cannot have regular pizza without doing serious damage to their body. \",\n          \"Student, had a test today and couldn't get to the plasma donation center in time so I could eat today.  A pizza would really hit the spot and tide me over until I can get some cash in the next few days and pay it forward.  Thanks for any considerations or help.\",\n          \"Just 3 college age kids who don't get paid for another week, contrary to what we were told by our boss\\nEdit: Sorry for no location, we are in Michigan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"requester_received_pizza\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "940b7bc059fb428bd08e6f1930b74f4e6a4d4823",
        "_cell_guid": "0c50ef1b-d0f5-480e-81cd-ae2922211564",
        "id": "V9fJFpD4NoFw"
      },
      "cell_type": "markdown",
      "source": [
        "# Lets take a look at the test data"
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "d7edfb13cfc94358fdb89455626d7b8d68adf7d9",
        "_cell_guid": "28c0923b-0ea3-456e-8311-3773a169684c",
        "collapsed": true,
        "id": "zBNIwJosNoFw"
      },
      "cell_type": "code",
      "execution_count": 72,
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/UNED/notebooks/test.json') as fin:\n",
        "    testjson = json.load(fin)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/UNED/notebooks/test.json') as fin:\n",
        "    testjson = json.load(fin)\n",
        "\n",
        "print('UID:\\t', testjson[0]['request_id'], '\\n')\n",
        "print('Title:\\t', testjson[0]['request_title'], '\\n')\n",
        "print('Text:\\t', testjson[0]['request_text_edit_aware'], '\\n')\n",
        "\n",
        "# Check if 'requester_received_pizza' key exists before accessing it\n",
        "if 'requester_received_pizza' in testjson[0]:\n",
        "    print('Tag:\\t', testjson[0]['requester_received_pizza'], end='\\n')\n",
        "else:\n",
        "    print(\"Tag: 'requester_received_pizza' key not found in this dataset.\", end='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG6evrNYIa1n",
        "outputId": "695caa9f-9691-409a-c647-d5850e355eef"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UID:\t t3_i8iy4 \n",
            "\n",
            "Title:\t [request] pregger gf 95 degree house and no food.. promise to pay it forward! Northern Colorado \n",
            "\n",
            "Text:\t Hey all! It's about 95 degrees here and our kitchen is pretty much empty save for some bread and cereal.  My girlfriend/fiance is 8 1/2 months pregnant and we could use a good meal.  We promise to pay it forward when we get money! Thanks so much in advance! \n",
            "\n",
            "Tag: 'requester_received_pizza' key not found in this dataset.\n"
          ]
        }
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UID:\t t3_i8iy4 \n",
            "\n",
            "Title:\t [request] pregger gf 95 degree house and no food.. promise to pay it forward! Northern Colorado \n",
            "\n",
            "Text:\t Hey all! It's about 95 degrees here and our kitchen is pretty much empty save for some bread and cereal.  My girlfriend/fiance is 8 1/2 months pregnant and we could use a good meal.  We promise to pay it forward when we get money! Thanks so much in advance! \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'requester_received_pizza'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-9cc2a9169db9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Title:\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Text:\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request_text_edit_aware'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tag:\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'requester_received_pizza'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'requester_received_pizza'"
          ]
        }
      ],
      "metadata": {
        "_uuid": "e3a07d7af63044bd190dbe630de39fccfca61740",
        "_cell_guid": "775eec1e-2d63-4dd9-8f79-e11ae53a04e5",
        "collapsed": true,
        "id": "WD3MIWMbNoFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "45345161-00d7-46ca-a0e4-5f5081a147ab"
      },
      "cell_type": "code",
      "execution_count": 73,
      "source": [
        "print('UID:\\t', testjson[0]['request_id'], '\\n')\n",
        "print('Title:\\t', testjson[0]['request_title'], '\\n')\n",
        "print('Text:\\t', testjson[0]['request_text_edit_aware'], '\\n')\n",
        "print('Tag:\\t', testjson[0]['requester_received_pizza'], end='\\n')"
      ]
    },
    {
      "metadata": {
        "_uuid": "df6e837ff69fbab2af2f47acffa0cb8d12f995a9",
        "_cell_guid": "6cb14f9e-0c8e-40d3-aff7-e542790336eb",
        "id": "HmvYbSRNNoFx"
      },
      "cell_type": "markdown",
      "source": [
        "# Gotcha again!\n",
        "\n",
        "In the test data, our label (i.e. `requester_received_pizza`) **won't be known** to us since that's the thing that our classifier is predicting.\n",
        "\n",
        "**Note:** Whatever features that we're going to train our classifier with, we should have them in our test set too. In our case we need to make sure that the test set has `request_text_edit_aware` field."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En los datos de prueba, no conoceremos nuestra etiqueta (es decir, requester_received_pizza) ya que eso es lo que nuestro clasificador está prediciendo.\n",
        "\n",
        "Nota: Independientemente de las características con las que vayamos a entrenar a nuestro clasificador, también deberíamos tenerlas en nuestro conjunto de prueba. En nuestro caso, debemos asegurarnos de que el conjunto de prueba tenga el campo request_text_edit_aware."
      ],
      "metadata": {
        "id": "wDBFR8KfImuu"
      }
    },
    {
      "metadata": {
        "_uuid": "aed441bb12db2d224cbea93c835ae1d1d27e363d",
        "_cell_guid": "6e4c5394-7c84-47e5-b859-9a60ff97f851",
        "id": "8XBmIm1xNoFx"
      },
      "cell_type": "markdown",
      "source": [
        "# Lets put the test data into a pandas DataFrame too"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Use pd.json_normalize instead of pd.io.json.json_normalize\n",
        "df = pd.json_normalize(testjson)  # Pandas magic...\n",
        "df_test = df[['request_id', 'request_title',\n",
        "               'request_text_edit_aware']]\n",
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6KCF67cmI01u",
        "outputId": "97345df6-1982-4554-ab1f-75677df4a5fa"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  request_id                                      request_title  \\\n",
              "0   t3_i8iy4  [request] pregger gf 95 degree house and no fo...   \n",
              "1  t3_1mfqi0  [Request] Lost my job day after labour day, st...   \n",
              "2   t3_lclka                (Request) pizza for my kids please?   \n",
              "3  t3_1jdgdj  [Request] Just moved to a new state(Waltham MA...   \n",
              "4   t3_t2qt4  [Request] Two girls in between paychecks, we'v...   \n",
              "\n",
              "                             request_text_edit_aware  \n",
              "0  Hey all! It's about 95 degrees here and our ki...  \n",
              "1  I didn't know a place like this exists! \\n\\nI ...  \n",
              "2  Hi Reddit. Im a single dad having a really rou...  \n",
              "3  Hi I just moved to Waltham MA from my home sta...  \n",
              "4  We're just sitting here near indianapolis on o...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fef479a1-a503-4b9d-97e3-c66320487350\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>request_id</th>\n",
              "      <th>request_title</th>\n",
              "      <th>request_text_edit_aware</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>t3_i8iy4</td>\n",
              "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
              "      <td>Hey all! It's about 95 degrees here and our ki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>t3_1mfqi0</td>\n",
              "      <td>[Request] Lost my job day after labour day, st...</td>\n",
              "      <td>I didn't know a place like this exists! \\n\\nI ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t3_lclka</td>\n",
              "      <td>(Request) pizza for my kids please?</td>\n",
              "      <td>Hi Reddit. Im a single dad having a really rou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>t3_1jdgdj</td>\n",
              "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
              "      <td>Hi I just moved to Waltham MA from my home sta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>t3_t2qt4</td>\n",
              "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
              "      <td>We're just sitting here near indianapolis on o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fef479a1-a503-4b9d-97e3-c66320487350')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fef479a1-a503-4b9d-97e3-c66320487350 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fef479a1-a503-4b9d-97e3-c66320487350');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e13ac31c-8d0e-44a0-8ac8-bdb071ecb167\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e13ac31c-8d0e-44a0-8ac8-bdb071ecb167')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e13ac31c-8d0e-44a0-8ac8-bdb071ecb167 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_test",
              "summary": "{\n  \"name\": \"df_test\",\n  \"rows\": 1631,\n  \"fields\": [\n    {\n      \"column\": \"request_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1631,\n        \"samples\": [\n          \"t3_xsnoi\",\n          \"t3_wj4na\",\n          \"t3_1i8cqz\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"request_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1629,\n        \"samples\": [\n          \"[Request] Suffered a bad back spasm and am having trouble moving about. Really would prefer delivery tonight instead of trying to cook.\",\n          \"[REQUEST] Texas| Company terminated hundred of contracts on Monday, barely slept all week and did not eat all day yesterday. Craving pizza.\",\n          \"[Request] Two starving room mates need some food.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"request_text_edit_aware\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1578,\n        \"samples\": [\n          \"Hey, \\nI live in Moscow, ID and had a ton of bills this week that ate up all of my funds, I won't have anymore money until around the first, and I am completely out of food...little help please?\",\n          \"I'll keep my sob story short.\\n\\nI'm not poor, though my current circumstances might lead one to believe that to be the case. I'm in a rut financially, trying to dig myself out with a newly acquired job. This would be the happy ending to my story, if I could see my pay checks. Instead, for the time being, I'll be feeding my checks directly to my landlord. \\n\\nSo I ask Reddit, with a humble heart and an empty stomach to help me out with a delicious pizza pie.\",\n          \"I can give you drawings, upvote your stuff, write you a poem, idk, whatever. I have precious little laptop energy left before it dies. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a2cacb2270490a92ec9a7e920e8ca17c730c1e09",
        "_cell_guid": "b869481f-c713-4aa6-aa0d-f2e5f4b2abfc",
        "id": "-JaCqqtjNoFx"
      },
      "cell_type": "markdown",
      "source": [
        "# Split training data before vectorization\n",
        "\n",
        "The first thing to do is to split our training data into 2 parts:\n",
        "\n",
        " - **training**: Use for training our model\n",
        " - **validation**: Use to check the \"soundness\" of our model\n",
        "\n",
        "**Note:**\n",
        "\n",
        " - Splitting the data into 2 parts and holding out one part to check the model is one of method to validate the \"soundness\" of our model. It's call the **hold-out** validation.\n",
        "\n",
        " - Another popular validation method is **cross-validation**, it's out of scope here but you can take a look at `crossvalidation` in `scikit-learn`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo primero que debemos hacer es dividir nuestros datos de entrenamiento en dos partes:\n",
        "\n",
        "entrenamiento: se utiliza para entrenar nuestro modelo\n",
        "validación: se utiliza para comprobar la \"solidez\" de nuestro modelo\n",
        "Nota:\n",
        "\n",
        "Dividir los datos en dos partes y dejar una parte para comprobar el modelo es uno de los métodos para validar la \"solidez\" de nuestro modelo. Se denomina validación de retención.\n",
        "\n",
        "Otro método de validación popular es la validación cruzada, que no se aborda aquí, pero puedes echarle un vistazo a la validación cruzada en scikit-learn.\n"
      ],
      "metadata": {
        "id": "g0yleYcII9jJ"
      }
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "ac1e8766dcf5d7d81fe4fd3a95e7bc659fbf1978",
        "_cell_guid": "0b61f317-079b-4253-b94a-a87b71bd47fb",
        "collapsed": true,
        "id": "qwjCAHSENoFx"
      },
      "cell_type": "code",
      "execution_count": 77,
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# It doesn't really matter what the function name is called\n",
        "# but the `train_test_split` is splitting up the data into\n",
        "# 2 parts according to the `test_size` argument you've set.\n",
        "\n",
        "# When we're splitting up the training data, we're spltting up\n",
        "# into train, valid split. The function name is just a name =)\n",
        "train, valid = train_test_split(df_train, test_size=0.2)"
      ]
    },
    {
      "metadata": {
        "_uuid": "f97576104bbabfc720fa0794ab6a8fc4e043f109",
        "_cell_guid": "8677912b-d9e2-4fd8-b722-acb3320af4fc",
        "id": "eEbykbbmNoFx"
      },
      "cell_type": "markdown",
      "source": [
        "# Vectorize the train and validation set"
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "4549079bfedbbff679ccde5f083b3c6814e73233",
        "_cell_guid": "02efe9a8-00e4-41fa-9b6a-5fbebe751feb",
        "collapsed": true,
        "id": "K0h_BhAUNoFx"
      },
      "cell_type": "code",
      "execution_count": 78,
      "source": [
        "# Initialize the vectorizer and\n",
        "# override the analyzer totally with the preprocess_text().\n",
        "# Note: the vectorizer is just an 'empty' object now.\n",
        "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
        "\n",
        "# When we use `CounterVectorizer.fit_transform`,\n",
        "# we essentially create the dictionary and\n",
        "# vectorize our input text at the same time.\n",
        "train_set = count_vect.fit_transform(train['request_text_edit_aware'])\n",
        "train_tags = train['requester_received_pizza']\n",
        "\n",
        "# When vectorizing the validation data, we use `CountVectorizer.transform()`.\n",
        "valid_set = count_vect.transform(valid['request_text_edit_aware'])\n",
        "valid_tags = valid['requester_received_pizza']"
      ]
    },
    {
      "metadata": {
        "_uuid": "eb879f0ea99c654625d7e4526de4c1f5602619ac",
        "_cell_guid": "85863807-de62-471c-a5f9-7ada4e54e0bf",
        "id": "8Ns5MY0yNoFx"
      },
      "cell_type": "markdown",
      "source": [
        "# Now, we need to vectorize the test data too\n",
        "\n",
        "After we vectorize our data, the input to train the classifier would be the vectorized text.\n",
        "<br>When we predict the label with the trained mdoel, our input needs to be vectorized too.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de vectorizar nuestros datos, la entrada para capacitar al clasificador sería el texto vectorizado.\n",
        "Cuando predecimos la etiqueta con el MDOEL capacitado, nuestra entrada también debe ser vectorizada."
      ],
      "metadata": {
        "id": "tz3Y2eGDJLQb"
      }
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "502788a6150bc9c7256b097a4d5c04b4822ac3eb",
        "_cell_guid": "766b1529-5967-45e9-9736-1fb69bdc6460",
        "collapsed": true,
        "id": "lYjXkhcoNoFx"
      },
      "cell_type": "code",
      "execution_count": 79,
      "source": [
        "# When vectorizing the test data, we use `CountVectorizer.transform()`.\n",
        "test_set = count_vect.transform(df_test['request_text_edit_aware'])"
      ]
    },
    {
      "metadata": {
        "_uuid": "3aaf138395ccdfe7a29d89d1b58da9db96ae8b36",
        "_cell_guid": "9d7ae411-ae2c-4098-beb2-ac6c2dc267a9",
        "id": "S1yD9wykNoFy"
      },
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes classifier in sklearn\n",
        "\n",
        "There are different variants of Naive Bayes (NB) classifier in `sklearn`. <br>\n",
        "For simplicity, lets just use the `MultinomialNB`.\n",
        "\n",
        "**Multinomial** is a big word but it just means many classes/categories/bins/boxes that needs to be classified."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existen diferentes variantes del clasificador Naive Bayes (NB) en Sklearn.\n",
        "Para simplificar, usemos solo el multinomialnb.\n",
        "\n",
        "Multinomial es una palabra grande, pero solo significa muchas clases/categorías/contenedores/cajas que deben clasificarse."
      ],
      "metadata": {
        "id": "hWMCrFm_Jexf"
      }
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "metadata": {
        "_uuid": "44cee633360c7989a3d67716553ef4e2bc9eae87",
        "_cell_guid": "8a159538-c58a-485d-bc23-d7115ee55a77",
        "collapsed": true,
        "id": "9yjyipLtNoFy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "4e4a28cc-a47a-43fb-ea5f-6002631fab5a"
      },
      "cell_type": "code",
      "execution_count": 80,
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# To train the classifier, simple do\n",
        "clf.fit(train_set, train_tags)"
      ]
    },
    {
      "metadata": {
        "_uuid": "fbec702a6003287ea5d49049be532ee96b6d7a14",
        "_cell_guid": "8c2776b5-6515-4321-a7e7-f925534241d6",
        "id": "O9D2LdxQNoFy"
      },
      "cell_type": "markdown",
      "source": [
        "# Before we test our classifier on the test set, we get a sense of how good it is on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we test our classifier on the test set, we get a sense of how good it is on the validation set."
      ],
      "metadata": {
        "id": "Vof3TQ8nJkbO"
      }
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pizza reception accuracy = 73.63861386138613\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "b5f7b1ac26a30b568d3f5dec2a3f76c34a316ee5",
        "_cell_guid": "a0bba339-cd62-46e9-9106-55ef15000dc5",
        "collapsed": true,
        "id": "KOSsKaODNoFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca7bdc4-9274-47f7-926d-4fa69408dee4"
      },
      "cell_type": "code",
      "execution_count": 81,
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# To predict our tags (i.e. whether requesters get their pizza),\n",
        "# we feed the vectorized `test_set` to .predict()\n",
        "predictions_valid = clf.predict(valid_set)\n",
        "\n",
        "print('Pizza reception accuracy = {}'.format(\n",
        "        accuracy_score(predictions_valid, valid_tags) * 100)\n",
        "     )"
      ]
    },
    {
      "metadata": {
        "_uuid": "293f22791ce16ad0071afa625f7b6efa0601da7d",
        "_cell_guid": "24fed6a6-9b91-4192-92e1-1c6ac3264dab",
        "id": "mbOUmAeENoFy"
      },
      "cell_type": "markdown",
      "source": [
        "# Now lets use the full training data set and re-vectorize and retrain the classifier\n",
        "\n",
        "More data == better model (in most cases)"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "metadata": {
        "_uuid": "4a0c2bbe6b98000d71e1b2f9cd00bb01b156522e",
        "_cell_guid": "f738aa5b-8e1b-4885-8f3e-6d279f23d082",
        "collapsed": true,
        "id": "c6MIZSTMNoFz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "81b45085-a54d-4afd-fd2c-c371d8ec5306"
      },
      "cell_type": "code",
      "execution_count": 82,
      "source": [
        "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
        "\n",
        "full_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])\n",
        "full_tags = df_train['requester_received_pizza']\n",
        "\n",
        "# Note: We have to re-vectorize the test set since\n",
        "#       now our vectorizer is different using the full\n",
        "#       training set.\n",
        "test_set = count_vect.transform(df_test['request_text_edit_aware'])\n",
        "\n",
        "# To train the classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(full_train_set, full_tags)"
      ]
    },
    {
      "metadata": {
        "_uuid": "dbf9eb2a3dde29a26ae51df05f120ee26334d0c3",
        "_cell_guid": "6013d400-4a01-438a-9afe-b540b6ec78b9",
        "id": "29AcRsjnNoFz"
      },
      "cell_type": "markdown",
      "source": [
        "# Finally, we use the classifier to predict on the test set"
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "450827a713f7eaaa2d3c3504b038a513c27b3b20",
        "_cell_guid": "2fe3ff2b-5157-43b2-ad8d-55389ad28a0e",
        "collapsed": true,
        "id": "gtTOfH3mNoFz"
      },
      "cell_type": "code",
      "execution_count": 83,
      "source": [
        "# To predict our tags (i.e. whether requesters get their pizza),\n",
        "# we feed the vectorized `test_set` to .predict()\n",
        "predictions = clf.predict(test_set)"
      ]
    },
    {
      "metadata": {
        "_uuid": "51a804beaf030dbb8cbdfc320d6a26d86991d274",
        "_cell_guid": "ef6b8e68-46be-4944-b809-872ddeabf386",
        "id": "85Bf8EF8NoFz"
      },
      "cell_type": "markdown",
      "source": [
        "**Note:** Since we don't have the `requester_received_pizza` field in test data, we can't measure accuracy. But we can do some exploration as shown below."
      ]
    },
    {
      "metadata": {
        "_uuid": "e948c2b162d178cd674689adc1ce9dcdb426cbb5",
        "_cell_guid": "1ac9a825-8ffa-4ebd-a223-73521a27bc10",
        "id": "PXwB-dLJNoFz"
      },
      "cell_type": "markdown",
      "source": [
        "# From the training data, we had 24% pizza giving rate"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of 4040 requests, only 994 gets their pizzas, 24.603960396039604% success rate...\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "4ad28c27f351de96d40b68aed92217b17f71c542",
        "_cell_guid": "80300555-265f-4557-8b52-17c343a34c58",
        "collapsed": true,
        "id": "KhLOkIrNNoF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df8a11d-889d-4646-9246-767dacf4794d"
      },
      "cell_type": "code",
      "execution_count": 84,
      "source": [
        "success_rate = sum(df_train['requester_received_pizza']) / len(df_train) * 100\n",
        "print(str('Of {} requests, only {} gets their pizzas,'\n",
        "          ' {}% success rate...'.format(len(df_train),\n",
        "                                        sum(df_train['requester_received_pizza']),\n",
        "                                       success_rate)\n",
        "         )\n",
        "     )"
      ]
    },
    {
      "metadata": {
        "_uuid": "9058b0225d2f7e87486ffb5e31f710c55431a39c",
        "_cell_guid": "3301d9e0-544f-41d5-9c32-b2ea95b37871",
        "id": "UAAOwsmsNoF0"
      },
      "cell_type": "markdown",
      "source": [
        "# Lolz, our classifier is rather stingy..."
      ]
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of 1631 requests, only 55 gets their pizzas, 3.3721643163703248% success rate...\n"
          ]
        }
      ],
      "metadata": {
        "_uuid": "b6b08ebc7396962d4268858d27e7f1f082943891",
        "_cell_guid": "111c7d74-3516-455d-a13d-8a0af3d1b3c6",
        "collapsed": true,
        "id": "ZXEXb62bNoF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0624cb7d-24a4-4c40-b453-c02ff877be72"
      },
      "cell_type": "code",
      "execution_count": 85,
      "source": [
        "success_rate = sum(predictions) / len(predictions) * 100\n",
        "print(str('Of {} requests, only {} gets their pizzas,'\n",
        "          ' {}% success rate...'.format(len(predictions),\n",
        "                                        sum(predictions),\n",
        "                                       success_rate)\n",
        "         )\n",
        "     )"
      ]
    },
    {
      "metadata": {
        "_uuid": "8fbd1af6bdd2f1c3e1ff0cf65976fa3252e171cc",
        "_cell_guid": "8ed85498-5dd2-48bf-a379-1b84d237a42b",
        "id": "RWpiJxDDNoF0"
      },
      "cell_type": "markdown",
      "source": [
        "# How accurate is our count vectorization naive bayes classifier on the test data?\n",
        "\n",
        "Since we don't have the `requester_received_pizza` field in the test data, we have to check that with an oracle (i.e. the person that knows).\n",
        "\n",
        "On Kaggle, **checking with the oracle** means uploading the file in the correct format and their script will process the scores and tell you how you did.\n",
        "\n",
        "**Note:** Different tasks will use different metrics but in most cases getting as many correct predictions as possible is the thing to aim for. We won't get into the details of how classifiers are evaluated but for a start, please see [precision, recall and F1-scores](https://en.wikipedia.org/wiki/Precision_and_recall)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de probar nuestro clasificador en el conjunto de pruebas, tenemos una idea de lo bueno que es en el conjunto de validación."
      ],
      "metadata": {
        "id": "rkCdfA3tJ1k7"
      }
    },
    {
      "metadata": {
        "_uuid": "9129b8f430bf2c652038a5a7316c0e9db1bb4f4a",
        "_cell_guid": "f8054679-9d04-4563-91d5-daa29d0f7ce6",
        "id": "OCSIoI_tNoF0"
      },
      "cell_type": "markdown",
      "source": [
        "# Finally, lets take a look at what format the oracle expects and create the output file for our predictions accordingly"
      ]
    },
    {
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../input/patching-pizzas/sampleSubmission.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-d7b9b100a354>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_sample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/patching-pizzas/sampleSubmission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_sample_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/patching-pizzas/sampleSubmission.csv'"
          ]
        }
      ],
      "metadata": {
        "_uuid": "66603c8033bf346a9cd418d2679c695e1973a820",
        "_cell_guid": "a683db0e-da0f-48b0-915c-c3bc288519f9",
        "collapsed": true,
        "id": "R2O5AxRTNoF0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "282dd58b-ba57-4424-b837-d0729526cc3b"
      },
      "cell_type": "code",
      "execution_count": 86,
      "source": [
        "df_sample_submission = pd.read_csv('../input/patching-pizzas/sampleSubmission.csv')\n",
        "df_sample_submission.head()"
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "1410d9b66337fc2924eb156277f059e116143919",
        "_cell_guid": "f28c1ce2-864d-4de3-926e-4dc317479216",
        "collapsed": true,
        "id": "UBXgoHudNoF0"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# We've kept the `request_id` previous in the `df_test` dataframe.\n",
        "# We can simply merge that column with our predictions.\n",
        "df_output = pd.DataFrame({'request_id': list(df_test['request_id']),\n",
        "                          'requester_received_pizza': list(predictions)}\n",
        "                        )\n",
        "# Convert the predictions from boolean to integer.\n",
        "df_output['requester_received_pizza'] = df_output['requester_received_pizza'].astype(int)\n",
        "df_output.head()"
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "e64b0d64cbea2a60b878ca0e445b27c722e368db",
        "_cell_guid": "4053487b-e0d6-48a3-ba95-dbe2465f78f7",
        "collapsed": true,
        "id": "9Cvr5mrWNoF0"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create the csv file.\n",
        "df_output.to_csv('basic-nlp-submission.csv')"
      ]
    },
    {
      "outputs": [],
      "metadata": {
        "_uuid": "dbbd128b7754fa488986036a55ce09ecbe52e167",
        "_cell_guid": "a97cb294-eb6c-479e-944a-b82e63dc008f",
        "collapsed": true,
        "id": "2ZfmU7k3NoF0"
      },
      "cell_type": "code",
      "execution_count": null,
      "source": []
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0
}